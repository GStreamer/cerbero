From f17e88c7eaaa58ce1e3f4f052b03ea2811e04533 Mon Sep 17 00:00:00 2001
From: Matthew Waters <matthew@centricular.com>
Date: Tue, 27 Mar 2018 19:10:26 +1100
Subject: [PATCH] asm: rename some assembly functions to not conflict with
 openssl

Otherwise including gnutls and openssl in the same library from static
components would produce duplicate symbols and fail to link.
---
 lib/accelerated/aarch64/elf/sha1-armv8.s         |   8 +-
 lib/accelerated/aarch64/elf/sha256-armv8.s       |   8 +-
 lib/accelerated/aarch64/elf/sha512-armv8.s       |   8 +-
 lib/accelerated/aarch64/sha-aarch64.c            |  12 +-
 lib/accelerated/x86/aes-cbc-x86-aesni.c          |   8 +-
 lib/accelerated/x86/aes-ccm-x86-aesni.c          |   4 +-
 lib/accelerated/x86/aes-gcm-x86-aesni.c          |   6 +-
 lib/accelerated/x86/aes-gcm-x86-pclmul-avx.c     |  12 +-
 lib/accelerated/x86/aes-gcm-x86-pclmul.c         |  12 +-
 lib/accelerated/x86/aes-x86.h                    |  10 +-
 lib/accelerated/x86/coff/aesni-x86.s             | 188 +++++++++++------------
 lib/accelerated/x86/coff/aesni-x86_64.s          |  70 ++++-----
 lib/accelerated/x86/coff/sha1-ssse3-x86.s        |   8 +-
 lib/accelerated/x86/coff/sha1-ssse3-x86_64.s     |  10 +-
 lib/accelerated/x86/coff/sha256-ssse3-x86.s      |   8 +-
 lib/accelerated/x86/coff/sha512-ssse3-x86_64.s   |  10 +-
 lib/accelerated/x86/elf/aesni-x86.s              |  94 ++++++------
 lib/accelerated/x86/elf/aesni-x86_64.s           |  88 +++++------
 lib/accelerated/x86/elf/sha1-ssse3-x86.s         |   8 +-
 lib/accelerated/x86/elf/sha1-ssse3-x86_64.s      |  14 +-
 lib/accelerated/x86/elf/sha256-ssse3-x86.s       |   8 +-
 lib/accelerated/x86/elf/sha512-ssse3-x86.s       |   8 +-
 lib/accelerated/x86/elf/sha512-ssse3-x86_64.s    |  14 +-
 lib/accelerated/x86/macosx/aesni-x86.s           | 152 +++++++++---------
 lib/accelerated/x86/macosx/aesni-x86_64.s        | 124 +++++++--------
 lib/accelerated/x86/macosx/sha1-ssse3-x86.s      |   6 +-
 lib/accelerated/x86/macosx/sha1-ssse3-x86_64.s   |   6 +-
 lib/accelerated/x86/macosx/sha256-ssse3-x86.s    |   6 +-
 lib/accelerated/x86/macosx/sha512-ssse3-x86_64.s |   6 +-
 lib/accelerated/x86/sha-x86-ssse3.c              |  12 +-
 30 files changed, 464 insertions(+), 464 deletions(-)

diff --git a/lib/accelerated/aarch64/elf/sha1-armv8.s b/lib/accelerated/aarch64/elf/sha1-armv8.s
index 0dc68fe..d24ac5c 100644
--- a/lib/accelerated/aarch64/elf/sha1-armv8.s
+++ b/lib/accelerated/aarch64/elf/sha1-armv8.s
@@ -47,10 +47,10 @@
 .text
 
 
-.globl sha1_block_data_order
-.type sha1_block_data_order,%function
+.globl _sha1_block_data_order
+.type _sha1_block_data_order,%function
 .align 6
-sha1_block_data_order:
+_sha1_block_data_order:
 
 
 
@@ -1116,7 +1116,7 @@ sha1_block_data_order:
  ldp x27,x28,[sp,#80]
  ldr x29,[sp],#96
  ret
-.size sha1_block_data_order,.-sha1_block_data_order
+.size _sha1_block_data_order,.-_sha1_block_data_order
 .type sha1_block_armv8,%function
 .align 6
 sha1_block_armv8:
diff --git a/lib/accelerated/aarch64/elf/sha256-armv8.s b/lib/accelerated/aarch64/elf/sha256-armv8.s
index c055ce4..4c11afb 100644
--- a/lib/accelerated/aarch64/elf/sha256-armv8.s
+++ b/lib/accelerated/aarch64/elf/sha256-armv8.s
@@ -47,10 +47,10 @@
 .text
 
 
-.globl sha256_block_data_order
-.type sha256_block_data_order,%function
+.globl _sha256_block_data_order
+.type _sha256_block_data_order,%function
 .align 6
-sha256_block_data_order:
+_sha256_block_data_order:
 
 
 
@@ -1021,7 +1021,7 @@ sha256_block_data_order:
  ldp x27,x28,[x29,#80]
  ldp x29,x30,[sp],#128
  ret
-.size sha256_block_data_order,.-sha256_block_data_order
+.size _sha256_block_data_order,.-_sha256_block_data_order
 
 .align 6
 .type .LK256,%object
diff --git a/lib/accelerated/aarch64/elf/sha512-armv8.s b/lib/accelerated/aarch64/elf/sha512-armv8.s
index 8484f27..ef654fc 100644
--- a/lib/accelerated/aarch64/elf/sha512-armv8.s
+++ b/lib/accelerated/aarch64/elf/sha512-armv8.s
@@ -47,10 +47,10 @@
 .text
 
 
-.globl sha512_block_data_order
-.type sha512_block_data_order,%function
+.globl _sha512_block_data_order
+.type _sha512_block_data_order,%function
 .align 6
-sha512_block_data_order:
+_sha512_block_data_order:
  stp x29,x30,[sp,#-128]!
  add x29,sp,#0
 
@@ -1011,7 +1011,7 @@ sha512_block_data_order:
  ldp x27,x28,[x29,#80]
  ldp x29,x30,[sp],#128
  ret
-.size sha512_block_data_order,.-sha512_block_data_order
+.size _sha512_block_data_order,.-_sha512_block_data_order
 
 .align 6
 .type .LK512,%object
diff --git a/lib/accelerated/aarch64/sha-aarch64.c b/lib/accelerated/aarch64/sha-aarch64.c
index 5b68204..e9a0b23 100644
--- a/lib/accelerated/aarch64/sha-aarch64.c
+++ b/lib/accelerated/aarch64/sha-aarch64.c
@@ -31,9 +31,9 @@
 #include <sha-aarch64.h>
 #include <aarch64-common.h>
 
-void sha1_block_data_order(void *c, const void *p, size_t len);
-void sha256_block_data_order(void *c, const void *p, size_t len);
-void sha512_block_data_order(void *c, const void *p, size_t len);
+void _sha1_block_data_order(void *c, const void *p, size_t len);
+void _sha256_block_data_order(void *c, const void *p, size_t len);
+void _sha512_block_data_order(void *c, const void *p, size_t len);
 
 typedef void (*update_func) (void *, size_t, const uint8_t *);
 typedef void (*digest_func) (void *, size_t, uint8_t *);
@@ -108,7 +108,7 @@ void aarch64_sha1_update(struct sha1_ctx *ctx, size_t length,
 
 		t2 = length / SHA1_DATA_SIZE;
 
-		sha1_block_data_order(&octx, data, t2);
+		_sha1_block_data_order(&octx, data, t2);
 
 		for (i=0;i<t2;i++)
 			ctx->count++;
@@ -161,7 +161,7 @@ void aarch64_sha256_update(struct sha256_ctx *ctx, size_t length,
 
 	if (length > 0) {
 		t2 = length / SHA1_DATA_SIZE;
-		sha256_block_data_order(&octx, data, t2);
+		_sha256_block_data_order(&octx, data, t2);
 		
 		for (i=0;i<t2;i++)
 			ctx->count++;
@@ -212,7 +212,7 @@ void aarch64_sha512_update(struct sha512_ctx *ctx, size_t length,
 
 	if (length > 0) {
 		t2 = length / SHA512_DATA_SIZE;
-		sha512_block_data_order(&octx, data, t2);
+		_sha512_block_data_order(&octx, data, t2);
 		
 		for (i=0;i<t2;i++)
 			MD_INCR(ctx);
diff --git a/lib/accelerated/x86/aes-cbc-x86-aesni.c b/lib/accelerated/x86/aes-cbc-x86-aesni.c
index 9b42cde..3c70b78 100644
--- a/lib/accelerated/x86/aes-cbc-x86-aesni.c
+++ b/lib/accelerated/x86/aes-cbc-x86-aesni.c
@@ -69,11 +69,11 @@ aes_cipher_setkey(void *_ctx, const void *userkey, size_t keysize)
 
 	if (ctx->enc)
 		ret =
-		    aesni_set_encrypt_key(userkey, keysize * 8,
+		    _aesni_set_encrypt_key(userkey, keysize * 8,
 					  ALIGN16(&ctx->expanded_key));
 	else
 		ret =
-		    aesni_set_decrypt_key(userkey, keysize * 8,
+		    _aesni_set_decrypt_key(userkey, keysize * 8,
 					  ALIGN16(&ctx->expanded_key));
 
 	if (ret != 0)
@@ -96,7 +96,7 @@ aes_encrypt(void *_ctx, const void *src, size_t src_size,
 {
 	struct aes_ctx *ctx = _ctx;
 
-	aesni_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
+	_aesni_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
 			  ctx->iv, 1);
 	return 0;
 }
@@ -107,7 +107,7 @@ aes_decrypt(void *_ctx, const void *src, size_t src_size,
 {
 	struct aes_ctx *ctx = _ctx;
 
-	aesni_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
+	_aesni_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
 			  ctx->iv, 0);
 
 	return 0;
diff --git a/lib/accelerated/x86/aes-ccm-x86-aesni.c b/lib/accelerated/x86/aes-ccm-x86-aesni.c
index 4828a22..69f0b3a 100644
--- a/lib/accelerated/x86/aes-ccm-x86-aesni.c
+++ b/lib/accelerated/x86/aes-ccm-x86-aesni.c
@@ -49,7 +49,7 @@ static void x86_aes_encrypt(const void *_ctx,
 			    const uint8_t * src)
 {
 	AES_KEY *ctx = (void*)_ctx;
-	aesni_ecb_encrypt(src, dst, length, ctx, 1);
+	_aesni_ecb_encrypt(src, dst, length, ctx, 1);
 }
 
 static int
@@ -76,7 +76,7 @@ static int
 aes_ccm_cipher_setkey(void *_ctx, const void *key, size_t length)
 {
 	struct ccm_x86_aes_ctx *ctx = _ctx;
-	aesni_set_encrypt_key(key, length*8, &ctx->key);
+	_aesni_set_encrypt_key(key, length*8, &ctx->key);
 
 	return 0;
 }
diff --git a/lib/accelerated/x86/aes-gcm-x86-aesni.c b/lib/accelerated/x86/aes-gcm-x86-aesni.c
index f361e70..c5869cd 100644
--- a/lib/accelerated/x86/aes-gcm-x86-aesni.c
+++ b/lib/accelerated/x86/aes-gcm-x86-aesni.c
@@ -49,7 +49,7 @@ static void x86_aes_encrypt(const void *_ctx,
 {
 	AES_KEY *ctx = (void*)_ctx;
 
-	aesni_ecb_encrypt(src, dst, length, ctx, 1);
+	_aesni_ecb_encrypt(src, dst, length, ctx, 1);
 }
 
 static void x86_aes128_set_encrypt_key(void *_ctx,
@@ -57,7 +57,7 @@ static void x86_aes128_set_encrypt_key(void *_ctx,
 {
 	AES_KEY *ctx = _ctx;
 
-	aesni_set_encrypt_key(key, 16*8, ctx);
+	_aesni_set_encrypt_key(key, 16*8, ctx);
 }
 
 static void x86_aes256_set_encrypt_key(void *_ctx,
@@ -65,7 +65,7 @@ static void x86_aes256_set_encrypt_key(void *_ctx,
 {
 	AES_KEY *ctx = _ctx;
 
-	aesni_set_encrypt_key(key, 32*8, ctx);
+	_aesni_set_encrypt_key(key, 32*8, ctx);
 }
 
 static int
diff --git a/lib/accelerated/x86/aes-gcm-x86-pclmul-avx.c b/lib/accelerated/x86/aes-gcm-x86-pclmul-avx.c
index 59cb7e8..287d4ec 100644
--- a/lib/accelerated/x86/aes-gcm-x86-pclmul-avx.c
+++ b/lib/accelerated/x86/aes-gcm-x86-pclmul-avx.c
@@ -100,12 +100,12 @@ aes_gcm_cipher_setkey(void *_ctx, const void *userkey, size_t keysize)
 	CHECK_AES_KEYSIZE(keysize);
 
 	ret =
-	    aesni_set_encrypt_key(userkey, keysize * 8,
+	    _aesni_set_encrypt_key(userkey, keysize * 8,
 				  ALIGN16(&ctx->expanded_key));
 	if (ret != 0)
 		return gnutls_assert_val(GNUTLS_E_ENCRYPTION_FAILED);
 
-	aesni_ecb_encrypt(ctx->gcm.H.c, ctx->gcm.H.c,
+	_aesni_ecb_encrypt(ctx->gcm.H.c, ctx->gcm.H.c,
 			  GCM_BLOCK_SIZE, ALIGN16(&ctx->expanded_key), 1);
 
 	ctx->gcm.H.u[0] = bswap_64(ctx->gcm.H.u[0]);
@@ -132,7 +132,7 @@ static int aes_gcm_setiv(void *_ctx, const void *iv, size_t iv_size)
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 2] = 0;
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 1] = 1;
 
-	aesni_ecb_encrypt(ctx->gcm.Yi.c, ctx->gcm.EK0.c,
+	_aesni_ecb_encrypt(ctx->gcm.Yi.c, ctx->gcm.EK0.c,
 			  GCM_BLOCK_SIZE, ALIGN16(&ctx->expanded_key), 1);
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 1] = 2;
 	return 0;
@@ -162,7 +162,7 @@ ctr_encrypt_last(struct aes_gcm_ctx *ctx, const uint8_t * src,
 	uint8_t out[GCM_BLOCK_SIZE];
 
 	memcpy(tmp, &src[pos], length);
-	aesni_ctr32_encrypt_blocks(tmp, out, 1,
+	_aesni_ctr32_encrypt_blocks(tmp, out, 1,
 				   ALIGN16(&ctx->expanded_key),
 				   ctx->gcm.Yi.c);
 
@@ -181,7 +181,7 @@ aes_gcm_encrypt(void *_ctx, const void *src, size_t src_size,
 	uint32_t counter;
 
 	if (blocks > 0) {
-		aesni_ctr32_encrypt_blocks(src, dst,
+		_aesni_ctr32_encrypt_blocks(src, dst,
 					   blocks,
 					   ALIGN16(&ctx->expanded_key),
 					   ctx->gcm.Yi.c);
@@ -214,7 +214,7 @@ aes_gcm_decrypt(void *_ctx, const void *src, size_t src_size,
 	ctx->gcm.len.u[1] += src_size;
 
 	if (blocks > 0) {
-		aesni_ctr32_encrypt_blocks(src, dst,
+		_aesni_ctr32_encrypt_blocks(src, dst,
 					   blocks,
 					   ALIGN16(&ctx->expanded_key),
 					   ctx->gcm.Yi.c);
diff --git a/lib/accelerated/x86/aes-gcm-x86-pclmul.c b/lib/accelerated/x86/aes-gcm-x86-pclmul.c
index 4411e54..3db86a6 100644
--- a/lib/accelerated/x86/aes-gcm-x86-pclmul.c
+++ b/lib/accelerated/x86/aes-gcm-x86-pclmul.c
@@ -99,12 +99,12 @@ aes_gcm_cipher_setkey(void *_ctx, const void *userkey, size_t keysize)
 	CHECK_AES_KEYSIZE(keysize);
 
 	ret =
-	    aesni_set_encrypt_key(userkey, keysize * 8,
+	    _aesni_set_encrypt_key(userkey, keysize * 8,
 				  ALIGN16(&ctx->expanded_key));
 	if (ret != 0)
 		return gnutls_assert_val(GNUTLS_E_ENCRYPTION_FAILED);
 
-	aesni_ecb_encrypt(ctx->gcm.H.c, ctx->gcm.H.c,
+	_aesni_ecb_encrypt(ctx->gcm.H.c, ctx->gcm.H.c,
 			  GCM_BLOCK_SIZE, ALIGN16(&ctx->expanded_key), 1);
 
 	ctx->gcm.H.u[0] = bswap_64(ctx->gcm.H.u[0]);
@@ -131,7 +131,7 @@ static int aes_gcm_setiv(void *_ctx, const void *iv, size_t iv_size)
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 2] = 0;
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 1] = 1;
 
-	aesni_ecb_encrypt(ctx->gcm.Yi.c, ctx->gcm.EK0.c,
+	_aesni_ecb_encrypt(ctx->gcm.Yi.c, ctx->gcm.EK0.c,
 			  GCM_BLOCK_SIZE, ALIGN16(&ctx->expanded_key), 1);
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 1] = 2;
 	return 0;
@@ -161,7 +161,7 @@ ctr_encrypt_last(struct aes_gcm_ctx *ctx, const uint8_t * src,
 	uint8_t out[GCM_BLOCK_SIZE];
 
 	memcpy(tmp, &src[pos], length);
-	aesni_ctr32_encrypt_blocks(tmp, out, 1,
+	_aesni_ctr32_encrypt_blocks(tmp, out, 1,
 				   ALIGN16(&ctx->expanded_key),
 				   ctx->gcm.Yi.c);
 
@@ -180,7 +180,7 @@ aes_gcm_encrypt(void *_ctx, const void *src, size_t src_size,
 	uint32_t counter;
 
 	if (blocks > 0) {
-		aesni_ctr32_encrypt_blocks(src, dst,
+		_aesni_ctr32_encrypt_blocks(src, dst,
 					   blocks,
 					   ALIGN16(&ctx->expanded_key),
 					   ctx->gcm.Yi.c);
@@ -213,7 +213,7 @@ aes_gcm_decrypt(void *_ctx, const void *src, size_t src_size,
 	ctx->gcm.len.u[1] += src_size;
 
 	if (blocks > 0) {
-		aesni_ctr32_encrypt_blocks(src, dst,
+		_aesni_ctr32_encrypt_blocks(src, dst,
 					   blocks,
 					   ALIGN16(&ctx->expanded_key),
 					   ctx->gcm.Yi.c);
diff --git a/lib/accelerated/x86/aes-x86.h b/lib/accelerated/x86/aes-x86.h
index 2fcd890..afb46d2 100644
--- a/lib/accelerated/x86/aes-x86.h
+++ b/lib/accelerated/x86/aes-x86.h
@@ -22,18 +22,18 @@ typedef struct {
 	if (s != 16 && s != 24 && s != 32) \
 		return GNUTLS_E_INVALID_REQUEST
 
-void aesni_ecb_encrypt(const unsigned char *in, unsigned char *out,
+void _aesni_ecb_encrypt(const unsigned char *in, unsigned char *out,
 		       size_t len, const AES_KEY * key, int enc);
 
-void aesni_cbc_encrypt(const unsigned char *in, unsigned char *out,
+void _aesni_cbc_encrypt(const unsigned char *in, unsigned char *out,
 		       size_t len, const AES_KEY * key,
 		       unsigned char *ivec, const int enc);
-int aesni_set_decrypt_key(const unsigned char *userKey, const int bits,
+int _aesni_set_decrypt_key(const unsigned char *userKey, const int bits,
 			  AES_KEY * key);
-int aesni_set_encrypt_key(const unsigned char *userKey, const int bits,
+int _aesni_set_encrypt_key(const unsigned char *userKey, const int bits,
 			  AES_KEY * key);
 
-void aesni_ctr32_encrypt_blocks(const unsigned char *in,
+void _aesni_ctr32_encrypt_blocks(const unsigned char *in,
 				unsigned char *out,
 				size_t blocks,
 				const void *key,
diff --git a/lib/accelerated/x86/coff/aesni-x86.s b/lib/accelerated/x86/coff/aesni-x86.s
index 502be77..e482756 100644
--- a/lib/accelerated/x86/coff/aesni-x86.s
+++ b/lib/accelerated/x86/coff/aesni-x86.s
@@ -39,11 +39,11 @@
 #
 .file	"devel/perlasm/aesni-x86.s"
 .text
-.globl	_aesni_encrypt
-.def	_aesni_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__aesni_encrypt
+.def	__aesni_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_encrypt:
-.L_aesni_encrypt_begin:
+__aesni_encrypt:
+.L__aesni_encrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
 	movups	(%eax),%xmm2
@@ -62,11 +62,11 @@ _aesni_encrypt:
 .byte	102,15,56,221,209
 	movups	%xmm2,(%eax)
 	ret
-.globl	_aesni_decrypt
-.def	_aesni_decrypt;	.scl	2;	.type	32;	.endef
+.globl	__aesni_decrypt
+.def	__aesni_decrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_decrypt:
-.L_aesni_decrypt_begin:
+__aesni_decrypt:
+.L__aesni_decrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
 	movups	(%eax),%xmm2
@@ -85,9 +85,9 @@ _aesni_decrypt:
 .byte	102,15,56,223,209
 	movups	%xmm2,(%eax)
 	ret
-.def	__aesni_encrypt3;	.scl	3;	.type	32;	.endef
+.def	___aesni_encrypt3;	.scl	3;	.type	32;	.endef
 .align	16
-__aesni_encrypt3:
+___aesni_encrypt3:
 	movups	(%edx),%xmm0
 	shrl	$1,%ecx
 	movups	16(%edx),%xmm1
@@ -115,9 +115,9 @@ __aesni_encrypt3:
 .byte	102,15,56,221,216
 .byte	102,15,56,221,224
 	ret
-.def	__aesni_decrypt3;	.scl	3;	.type	32;	.endef
+.def	___aesni_decrypt3;	.scl	3;	.type	32;	.endef
 .align	16
-__aesni_decrypt3:
+___aesni_decrypt3:
 	movups	(%edx),%xmm0
 	shrl	$1,%ecx
 	movups	16(%edx),%xmm1
@@ -145,9 +145,9 @@ __aesni_decrypt3:
 .byte	102,15,56,223,216
 .byte	102,15,56,223,224
 	ret
-.def	__aesni_encrypt4;	.scl	3;	.type	32;	.endef
+.def	___aesni_encrypt4;	.scl	3;	.type	32;	.endef
 .align	16
-__aesni_encrypt4:
+___aesni_encrypt4:
 	movups	(%edx),%xmm0
 	movups	16(%edx),%xmm1
 	shrl	$1,%ecx
@@ -180,9 +180,9 @@ __aesni_encrypt4:
 .byte	102,15,56,221,224
 .byte	102,15,56,221,232
 	ret
-.def	__aesni_decrypt4;	.scl	3;	.type	32;	.endef
+.def	___aesni_decrypt4;	.scl	3;	.type	32;	.endef
 .align	16
-__aesni_decrypt4:
+___aesni_decrypt4:
 	movups	(%edx),%xmm0
 	movups	16(%edx),%xmm1
 	shrl	$1,%ecx
@@ -215,9 +215,9 @@ __aesni_decrypt4:
 .byte	102,15,56,223,224
 .byte	102,15,56,223,232
 	ret
-.def	__aesni_encrypt6;	.scl	3;	.type	32;	.endef
+.def	___aesni_encrypt6;	.scl	3;	.type	32;	.endef
 .align	16
-__aesni_encrypt6:
+___aesni_encrypt6:
 	movups	(%edx),%xmm0
 	shrl	$1,%ecx
 	movups	16(%edx),%xmm1
@@ -236,7 +236,7 @@ __aesni_encrypt6:
 .byte	102,15,56,220,241
 	movups	(%edx),%xmm0
 .byte	102,15,56,220,249
-	jmp	.L_aesni_encrypt6_enter
+	jmp	.L__aesni_encrypt6_enter
 .align	16
 .L006enc6_loop:
 .byte	102,15,56,220,209
@@ -247,7 +247,7 @@ __aesni_encrypt6:
 .byte	102,15,56,220,241
 .byte	102,15,56,220,249
 .align	16
-.L_aesni_encrypt6_enter:
+.L__aesni_encrypt6_enter:
 	movups	16(%edx),%xmm1
 .byte	102,15,56,220,208
 .byte	102,15,56,220,216
@@ -271,9 +271,9 @@ __aesni_encrypt6:
 .byte	102,15,56,221,240
 .byte	102,15,56,221,248
 	ret
-.def	__aesni_decrypt6;	.scl	3;	.type	32;	.endef
+.def	___aesni_decrypt6;	.scl	3;	.type	32;	.endef
 .align	16
-__aesni_decrypt6:
+___aesni_decrypt6:
 	movups	(%edx),%xmm0
 	shrl	$1,%ecx
 	movups	16(%edx),%xmm1
@@ -292,7 +292,7 @@ __aesni_decrypt6:
 .byte	102,15,56,222,241
 	movups	(%edx),%xmm0
 .byte	102,15,56,222,249
-	jmp	.L_aesni_decrypt6_enter
+	jmp	.L__aesni_decrypt6_enter
 .align	16
 .L007dec6_loop:
 .byte	102,15,56,222,209
@@ -303,7 +303,7 @@ __aesni_decrypt6:
 .byte	102,15,56,222,241
 .byte	102,15,56,222,249
 .align	16
-.L_aesni_decrypt6_enter:
+.L__aesni_decrypt6_enter:
 	movups	16(%edx),%xmm1
 .byte	102,15,56,222,208
 .byte	102,15,56,222,216
@@ -327,11 +327,11 @@ __aesni_decrypt6:
 .byte	102,15,56,223,240
 .byte	102,15,56,223,248
 	ret
-.globl	_aesni_ecb_encrypt
-.def	_aesni_ecb_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__aesni_ecb_encrypt
+.def	__aesni_ecb_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_ecb_encrypt:
-.L_aesni_ecb_encrypt_begin:
+__aesni_ecb_encrypt:
+.L__aesni_ecb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -376,7 +376,7 @@ _aesni_ecb_encrypt:
 	movdqu	80(%esi),%xmm7
 	leal	96(%esi),%esi
 .L011ecb_enc_loop6_enter:
-	call	__aesni_encrypt6
+	call	___aesni_encrypt6
 	movl	%ebp,%edx
 	movl	%ebx,%ecx
 	subl	$96,%eax
@@ -403,7 +403,7 @@ _aesni_ecb_encrypt:
 	je	.L016ecb_enc_four
 	movups	64(%esi),%xmm6
 	xorps	%xmm7,%xmm7
-	call	__aesni_encrypt6
+	call	___aesni_encrypt6
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
@@ -428,20 +428,20 @@ _aesni_ecb_encrypt:
 .align	16
 .L014ecb_enc_two:
 	xorps	%xmm4,%xmm4
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	jmp	.L008ecb_ret
 .align	16
 .L015ecb_enc_three:
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
 	jmp	.L008ecb_ret
 .align	16
 .L016ecb_enc_four:
-	call	__aesni_encrypt4
+	call	___aesni_encrypt4
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
@@ -479,7 +479,7 @@ _aesni_ecb_encrypt:
 	movdqu	80(%esi),%xmm7
 	leal	96(%esi),%esi
 .L019ecb_dec_loop6_enter:
-	call	__aesni_decrypt6
+	call	___aesni_decrypt6
 	movl	%ebp,%edx
 	movl	%ebx,%ecx
 	subl	$96,%eax
@@ -506,7 +506,7 @@ _aesni_ecb_encrypt:
 	je	.L024ecb_dec_four
 	movups	64(%esi),%xmm6
 	xorps	%xmm7,%xmm7
-	call	__aesni_decrypt6
+	call	___aesni_decrypt6
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
@@ -531,20 +531,20 @@ _aesni_ecb_encrypt:
 .align	16
 .L022ecb_dec_two:
 	xorps	%xmm4,%xmm4
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	jmp	.L008ecb_ret
 .align	16
 .L023ecb_dec_three:
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
 	jmp	.L008ecb_ret
 .align	16
 .L024ecb_dec_four:
-	call	__aesni_decrypt4
+	call	___aesni_decrypt4
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
@@ -555,11 +555,11 @@ _aesni_ecb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ccm64_encrypt_blocks
-.def	_aesni_ccm64_encrypt_blocks;	.scl	2;	.type	32;	.endef
+.globl	__aesni_ccm64_encrypt_blocks
+.def	__aesni_ccm64_encrypt_blocks;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_ccm64_encrypt_blocks:
-.L_aesni_ccm64_encrypt_blocks_begin:
+__aesni_ccm64_encrypt_blocks:
+.L__aesni_ccm64_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -634,11 +634,11 @@ _aesni_ccm64_encrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ccm64_decrypt_blocks
-.def	_aesni_ccm64_decrypt_blocks;	.scl	2;	.type	32;	.endef
+.globl	__aesni_ccm64_decrypt_blocks
+.def	__aesni_ccm64_decrypt_blocks;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_ccm64_decrypt_blocks:
-.L_aesni_ccm64_decrypt_blocks_begin:
+__aesni_ccm64_decrypt_blocks:
+.L__aesni_ccm64_decrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -745,11 +745,11 @@ _aesni_ccm64_decrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ctr32_encrypt_blocks
-.def	_aesni_ctr32_encrypt_blocks;	.scl	2;	.type	32;	.endef
+.globl	__aesni_ctr32_encrypt_blocks
+.def	__aesni_ctr32_encrypt_blocks;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_ctr32_encrypt_blocks:
-.L_aesni_ctr32_encrypt_blocks_begin:
+__aesni_ctr32_encrypt_blocks:
+.L__aesni_ctr32_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -838,7 +838,7 @@ _aesni_ctr32_encrypt_blocks:
 .byte	102,15,56,220,241
 	movups	(%edx),%xmm0
 .byte	102,15,56,220,249
-	call	.L_aesni_encrypt6_enter
+	call	.L__aesni_encrypt6_enter
 	movups	(%esi),%xmm1
 	movups	16(%esi),%xmm0
 	xorps	%xmm1,%xmm2
@@ -893,7 +893,7 @@ _aesni_ctr32_encrypt_blocks:
 	por	%xmm7,%xmm5
 	je	.L040ctr32_four
 	por	%xmm7,%xmm6
-	call	__aesni_encrypt6
+	call	___aesni_encrypt6
 	movups	(%esi),%xmm1
 	movups	16(%esi),%xmm0
 	xorps	%xmm1,%xmm2
@@ -932,7 +932,7 @@ _aesni_ctr32_encrypt_blocks:
 	jmp	.L036ctr32_ret
 .align	16
 .L038ctr32_two:
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	movups	(%esi),%xmm5
 	movups	16(%esi),%xmm6
 	xorps	%xmm5,%xmm2
@@ -942,7 +942,7 @@ _aesni_ctr32_encrypt_blocks:
 	jmp	.L036ctr32_ret
 .align	16
 .L039ctr32_three:
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	movups	(%esi),%xmm5
 	movups	16(%esi),%xmm6
 	xorps	%xmm5,%xmm2
@@ -955,7 +955,7 @@ _aesni_ctr32_encrypt_blocks:
 	jmp	.L036ctr32_ret
 .align	16
 .L040ctr32_four:
-	call	__aesni_encrypt4
+	call	___aesni_encrypt4
 	movups	(%esi),%xmm6
 	movups	16(%esi),%xmm7
 	movups	32(%esi),%xmm1
@@ -975,11 +975,11 @@ _aesni_ctr32_encrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_xts_encrypt
-.def	_aesni_xts_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__aesni_xts_encrypt
+.def	__aesni_xts_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_xts_encrypt:
-.L_aesni_xts_encrypt_begin:
+__aesni_xts_encrypt:
+.L__aesni_xts_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -1091,7 +1091,7 @@ _aesni_xts_encrypt:
 .byte	102,15,56,220,241
 	movups	(%edx),%xmm0
 .byte	102,15,56,220,249
-	call	.L_aesni_encrypt6_enter
+	call	.L__aesni_encrypt6_enter
 	movdqa	80(%esp),%xmm1
 	pxor	%xmm0,%xmm0
 	xorps	(%esp),%xmm2
@@ -1171,7 +1171,7 @@ _aesni_xts_encrypt:
 	pxor	48(%esp),%xmm5
 	movdqa	%xmm7,64(%esp)
 	pxor	%xmm7,%xmm6
-	call	__aesni_encrypt6
+	call	___aesni_encrypt6
 	movaps	64(%esp),%xmm1
 	xorps	(%esp),%xmm2
 	xorps	16(%esp),%xmm3
@@ -1215,7 +1215,7 @@ _aesni_xts_encrypt:
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm4,%xmm4
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	movups	%xmm2,(%edi)
@@ -1233,7 +1233,7 @@ _aesni_xts_encrypt:
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm7,%xmm4
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm7,%xmm4
@@ -1255,7 +1255,7 @@ _aesni_xts_encrypt:
 	xorps	16(%esp),%xmm3
 	xorps	%xmm7,%xmm4
 	xorps	%xmm6,%xmm5
-	call	__aesni_encrypt4
+	call	___aesni_encrypt4
 	xorps	(%esp),%xmm2
 	xorps	16(%esp),%xmm3
 	xorps	%xmm7,%xmm4
@@ -1321,11 +1321,11 @@ _aesni_xts_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_xts_decrypt
-.def	_aesni_xts_decrypt;	.scl	2;	.type	32;	.endef
+.globl	__aesni_xts_decrypt
+.def	__aesni_xts_decrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_xts_decrypt:
-.L_aesni_xts_decrypt_begin:
+__aesni_xts_decrypt:
+.L__aesni_xts_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -1442,7 +1442,7 @@ _aesni_xts_decrypt:
 .byte	102,15,56,222,241
 	movups	(%edx),%xmm0
 .byte	102,15,56,222,249
-	call	.L_aesni_decrypt6_enter
+	call	.L__aesni_decrypt6_enter
 	movdqa	80(%esp),%xmm1
 	pxor	%xmm0,%xmm0
 	xorps	(%esp),%xmm2
@@ -1522,7 +1522,7 @@ _aesni_xts_decrypt:
 	pxor	48(%esp),%xmm5
 	movdqa	%xmm7,64(%esp)
 	pxor	%xmm7,%xmm6
-	call	__aesni_decrypt6
+	call	___aesni_decrypt6
 	movaps	64(%esp),%xmm1
 	xorps	(%esp),%xmm2
 	xorps	16(%esp),%xmm3
@@ -1565,7 +1565,7 @@ _aesni_xts_decrypt:
 	leal	32(%esi),%esi
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	movups	%xmm2,(%edi)
@@ -1583,7 +1583,7 @@ _aesni_xts_decrypt:
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm7,%xmm4
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm7,%xmm4
@@ -1605,7 +1605,7 @@ _aesni_xts_decrypt:
 	xorps	16(%esp),%xmm3
 	xorps	%xmm7,%xmm4
 	xorps	%xmm6,%xmm5
-	call	__aesni_decrypt4
+	call	___aesni_decrypt4
 	xorps	(%esp),%xmm2
 	xorps	16(%esp),%xmm3
 	xorps	%xmm7,%xmm4
@@ -1696,11 +1696,11 @@ _aesni_xts_decrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_cbc_encrypt
-.def	_aesni_cbc_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__aesni_cbc_encrypt
+.def	__aesni_cbc_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_cbc_encrypt:
-.L_aesni_cbc_encrypt_begin:
+__aesni_cbc_encrypt:
+.L__aesni_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -1785,7 +1785,7 @@ _aesni_cbc_encrypt:
 	movdqu	48(%esi),%xmm5
 	movdqu	64(%esi),%xmm6
 	movdqu	80(%esi),%xmm7
-	call	__aesni_decrypt6
+	call	___aesni_decrypt6
 	movups	(%esi),%xmm1
 	movups	16(%esi),%xmm0
 	xorps	(%esp),%xmm2
@@ -1834,7 +1834,7 @@ _aesni_cbc_encrypt:
 	movaps	%xmm7,(%esp)
 	movups	(%esi),%xmm2
 	xorps	%xmm7,%xmm7
-	call	__aesni_decrypt6
+	call	___aesni_decrypt6
 	movups	(%esi),%xmm1
 	movups	16(%esi),%xmm0
 	xorps	(%esp),%xmm2
@@ -1873,7 +1873,7 @@ _aesni_cbc_encrypt:
 .align	16
 .L081cbc_dec_two:
 	xorps	%xmm4,%xmm4
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	xorps	%xmm7,%xmm2
 	xorps	%xmm6,%xmm3
 	movups	%xmm2,(%edi)
@@ -1884,7 +1884,7 @@ _aesni_cbc_encrypt:
 	jmp	.L079cbc_dec_tail_collected
 .align	16
 .L082cbc_dec_three:
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	xorps	%xmm7,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm5,%xmm4
@@ -1897,7 +1897,7 @@ _aesni_cbc_encrypt:
 	jmp	.L079cbc_dec_tail_collected
 .align	16
 .L083cbc_dec_four:
-	call	__aesni_decrypt4
+	call	___aesni_decrypt4
 	movups	16(%esi),%xmm1
 	movups	32(%esi),%xmm0
 	xorps	%xmm7,%xmm2
@@ -1933,9 +1933,9 @@ _aesni_cbc_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.def	__aesni_set_encrypt_key;	.scl	3;	.type	32;	.endef
+.def	___aesni_set_encrypt_key;	.scl	3;	.type	32;	.endef
 .align	16
-__aesni_set_encrypt_key:
+___aesni_set_encrypt_key:
 	testl	%eax,%eax
 	jz	.L086bad_pointer
 	testl	%edx,%edx
@@ -2111,25 +2111,25 @@ __aesni_set_encrypt_key:
 .L089bad_keybits:
 	movl	$-2,%eax
 	ret
-.globl	_aesni_set_encrypt_key
-.def	_aesni_set_encrypt_key;	.scl	2;	.type	32;	.endef
+.globl	__aesni_set_encrypt_key
+.def	__aesni_set_encrypt_key;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_set_encrypt_key:
-.L_aesni_set_encrypt_key_begin:
+__aesni_set_encrypt_key:
+.L__aesni_set_encrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
 	movl	12(%esp),%edx
-	call	__aesni_set_encrypt_key
+	call	___aesni_set_encrypt_key
 	ret
-.globl	_aesni_set_decrypt_key
-.def	_aesni_set_decrypt_key;	.scl	2;	.type	32;	.endef
+.globl	__aesni_set_decrypt_key
+.def	__aesni_set_decrypt_key;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_set_decrypt_key:
-.L_aesni_set_decrypt_key_begin:
+__aesni_set_decrypt_key:
+.L__aesni_set_decrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
 	movl	12(%esp),%edx
-	call	__aesni_set_encrypt_key
+	call	___aesni_set_encrypt_key
 	movl	12(%esp),%edx
 	shll	$4,%ecx
 	testl	%eax,%eax
diff --git a/lib/accelerated/x86/coff/aesni-x86_64.s b/lib/accelerated/x86/coff/aesni-x86_64.s
index 79ffbf7..85773f8 100644
--- a/lib/accelerated/x86/coff/aesni-x86_64.s
+++ b/lib/accelerated/x86/coff/aesni-x86_64.s
@@ -39,10 +39,10 @@
 #
 .text	
 
-.globl	aesni_encrypt
-.def	aesni_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_aesni_encrypt
+.def	_aesni_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_encrypt:
+_aesni_encrypt:
 	movups	(%rcx),%xmm2
 	movl	240(%r8),%eax
 	movups	(%r8),%xmm0
@@ -63,10 +63,10 @@ aesni_encrypt:
 	.byte	0xf3,0xc3
 
 
-.globl	aesni_decrypt
-.def	aesni_decrypt;	.scl 2;	.type 32;	.endef
+.globl	_aesni_decrypt
+.def	_aesni_decrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_decrypt:
+_aesni_decrypt:
 	movups	(%rcx),%xmm2
 	movl	240(%r8),%eax
 	movups	(%r8),%xmm0
@@ -528,10 +528,10 @@ _aesni_decrypt8:
 .byte	102,68,15,56,223,200
 	.byte	0xf3,0xc3
 
-.globl	aesni_ecb_encrypt
-.def	aesni_ecb_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_aesni_ecb_encrypt
+.def	_aesni_ecb_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_ecb_encrypt:
+_aesni_ecb_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -898,10 +898,10 @@ aesni_ecb_encrypt:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_aesni_ecb_encrypt:
-.globl	aesni_ccm64_encrypt_blocks
-.def	aesni_ccm64_encrypt_blocks;	.scl 2;	.type 32;	.endef
+.globl	_aesni_ccm64_encrypt_blocks
+.def	_aesni_ccm64_encrypt_blocks;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_ccm64_encrypt_blocks:
+_aesni_ccm64_encrypt_blocks:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -990,10 +990,10 @@ aesni_ccm64_encrypt_blocks:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_aesni_ccm64_encrypt_blocks:
-.globl	aesni_ccm64_decrypt_blocks
-.def	aesni_ccm64_decrypt_blocks;	.scl 2;	.type 32;	.endef
+.globl	_aesni_ccm64_decrypt_blocks
+.def	_aesni_ccm64_decrypt_blocks;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_ccm64_decrypt_blocks:
+_aesni_ccm64_decrypt_blocks:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -1116,10 +1116,10 @@ aesni_ccm64_decrypt_blocks:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_aesni_ccm64_decrypt_blocks:
-.globl	aesni_ctr32_encrypt_blocks
-.def	aesni_ctr32_encrypt_blocks;	.scl 2;	.type 32;	.endef
+.globl	_aesni_ctr32_encrypt_blocks
+.def	_aesni_ctr32_encrypt_blocks;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_ctr32_encrypt_blocks:
+_aesni_ctr32_encrypt_blocks:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -1722,10 +1722,10 @@ aesni_ctr32_encrypt_blocks:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_aesni_ctr32_encrypt_blocks:
-.globl	aesni_xts_encrypt
-.def	aesni_xts_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_aesni_xts_encrypt
+.def	_aesni_xts_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_xts_encrypt:
+_aesni_xts_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -2221,10 +2221,10 @@ aesni_xts_encrypt:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_aesni_xts_encrypt:
-.globl	aesni_xts_decrypt
-.def	aesni_xts_decrypt;	.scl 2;	.type 32;	.endef
+.globl	_aesni_xts_decrypt
+.def	_aesni_xts_decrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_xts_decrypt:
+_aesni_xts_decrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -2757,10 +2757,10 @@ aesni_xts_decrypt:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_aesni_xts_decrypt:
-.globl	aesni_cbc_encrypt
-.def	aesni_cbc_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_aesni_cbc_encrypt
+.def	_aesni_cbc_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_cbc_encrypt:
+_aesni_cbc_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -3382,10 +3382,10 @@ aesni_cbc_encrypt:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_aesni_cbc_encrypt:
-.globl	aesni_set_decrypt_key
-.def	aesni_set_decrypt_key;	.scl 2;	.type 32;	.endef
+.globl	_aesni_set_decrypt_key
+.def	_aesni_set_decrypt_key;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_set_decrypt_key:
+_aesni_set_decrypt_key:
 .byte	0x48,0x83,0xEC,0x08
 	call	__aesni_set_encrypt_key
 	shll	$4,%edx
@@ -3422,10 +3422,10 @@ aesni_set_decrypt_key:
 	.byte	0xf3,0xc3
 .LSEH_end_set_decrypt_key:
 
-.globl	aesni_set_encrypt_key
-.def	aesni_set_encrypt_key;	.scl 2;	.type 32;	.endef
+.globl	_aesni_set_encrypt_key
+.def	_aesni_set_encrypt_key;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_set_encrypt_key:
+_aesni_set_encrypt_key:
 __aesni_set_encrypt_key:
 .byte	0x48,0x83,0xEC,0x08
 	movq	$-1,%rax
@@ -4010,11 +4010,11 @@ cbc_se_handler:
 .rva	.LSEH_end_aesni_cbc_encrypt
 .rva	.LSEH_info_cbc
 
-.rva	aesni_set_decrypt_key
+.rva	_aesni_set_decrypt_key
 .rva	.LSEH_end_set_decrypt_key
 .rva	.LSEH_info_key
 
-.rva	aesni_set_encrypt_key
+.rva	_aesni_set_encrypt_key
 .rva	.LSEH_end_set_encrypt_key
 .rva	.LSEH_info_key
 .section	.xdata
diff --git a/lib/accelerated/x86/coff/sha1-ssse3-x86.s b/lib/accelerated/x86/coff/sha1-ssse3-x86.s
index 22c17e7..0e4db9d 100644
--- a/lib/accelerated/x86/coff/sha1-ssse3-x86.s
+++ b/lib/accelerated/x86/coff/sha1-ssse3-x86.s
@@ -39,11 +39,11 @@
 #
 .file	"sha1-586.s"
 .text
-.globl	_sha1_block_data_order
-.def	_sha1_block_data_order;	.scl	2;	.type	32;	.endef
+.globl	__sha1_block_data_order
+.def	__sha1_block_data_order;	.scl	2;	.type	32;	.endef
 .align	16
-_sha1_block_data_order:
-.L_sha1_block_data_order_begin:
+__sha1_block_data_order:
+.L__sha1_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
diff --git a/lib/accelerated/x86/coff/sha1-ssse3-x86_64.s b/lib/accelerated/x86/coff/sha1-ssse3-x86_64.s
index 13203c2..1a6c904 100644
--- a/lib/accelerated/x86/coff/sha1-ssse3-x86_64.s
+++ b/lib/accelerated/x86/coff/sha1-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	sha1_block_data_order
-.def	sha1_block_data_order;	.scl 2;	.type 32;	.endef
+.globl	_sha1_block_data_order
+.def	_sha1_block_data_order;	.scl 2;	.type 32;	.endef
 .p2align	4
-sha1_block_data_order:
+_sha1_block_data_order:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -1337,9 +1337,9 @@ sha1_block_data_order:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_sha1_block_data_order:
-.def	sha1_block_data_order_ssse3;	.scl 3;	.type 32;	.endef
+.def	_sha1_block_data_order_ssse3;	.scl 3;	.type 32;	.endef
 .p2align	4
-sha1_block_data_order_ssse3:
+_sha1_block_data_order_ssse3:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
diff --git a/lib/accelerated/x86/coff/sha256-ssse3-x86.s b/lib/accelerated/x86/coff/sha256-ssse3-x86.s
index eaa4354..ccfa84c 100644
--- a/lib/accelerated/x86/coff/sha256-ssse3-x86.s
+++ b/lib/accelerated/x86/coff/sha256-ssse3-x86.s
@@ -39,11 +39,11 @@
 #
 .file	"sha512-586.s"
 .text
-.globl	_sha256_block_data_order
-.def	_sha256_block_data_order;	.scl	2;	.type	32;	.endef
+.globl	__sha256_block_data_order
+.def	__sha256_block_data_order;	.scl	2;	.type	32;	.endef
 .align	16
-_sha256_block_data_order:
-.L_sha256_block_data_order_begin:
+__sha256_block_data_order:
+.L__sha256_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
diff --git a/lib/accelerated/x86/coff/sha512-ssse3-x86_64.s b/lib/accelerated/x86/coff/sha512-ssse3-x86_64.s
index 034dab2..8eea2e9 100644
--- a/lib/accelerated/x86/coff/sha512-ssse3-x86_64.s
+++ b/lib/accelerated/x86/coff/sha512-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	sha256_block_data_order
-.def	sha256_block_data_order;	.scl 2;	.type 32;	.endef
+.globl	_sha256_block_data_order
+.def	_sha256_block_data_order;	.scl 2;	.type 32;	.endef
 .p2align	4
-sha256_block_data_order:
+_sha256_block_data_order:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -1809,9 +1809,9 @@ K256:
 .long	0xffffffff,0xffffffff,0x03020100,0x0b0a0908
 .long	0xffffffff,0xffffffff,0x03020100,0x0b0a0908
 .byte	83,72,65,50,53,54,32,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
-.def	sha256_block_data_order_ssse3;	.scl 3;	.type 32;	.endef
+.def	_sha256_block_data_order_ssse3;	.scl 3;	.type 32;	.endef
 .p2align	6
-sha256_block_data_order_ssse3:
+_sha256_block_data_order_ssse3:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
diff --git a/lib/accelerated/x86/elf/aesni-x86.s b/lib/accelerated/x86/elf/aesni-x86.s
index 5d70f25..1032802 100644
--- a/lib/accelerated/x86/elf/aesni-x86.s
+++ b/lib/accelerated/x86/elf/aesni-x86.s
@@ -39,10 +39,10 @@
 #
 .file	"devel/perlasm/aesni-x86.s"
 .text
-.globl	aesni_encrypt
-.type	aesni_encrypt,@function
+.globl	_aesni_encrypt
+.type	_aesni_encrypt,@function
 .align	16
-aesni_encrypt:
+_aesni_encrypt:
 .L_aesni_encrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
@@ -62,11 +62,11 @@ aesni_encrypt:
 .byte	102,15,56,221,209
 	movups	%xmm2,(%eax)
 	ret
-.size	aesni_encrypt,.-.L_aesni_encrypt_begin
-.globl	aesni_decrypt
-.type	aesni_decrypt,@function
+.size	_aesni_encrypt,.-.L_aesni_encrypt_begin
+.globl	_aesni_decrypt
+.type	_aesni_decrypt,@function
 .align	16
-aesni_decrypt:
+_aesni_decrypt:
 .L_aesni_decrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
@@ -86,7 +86,7 @@ aesni_decrypt:
 .byte	102,15,56,223,209
 	movups	%xmm2,(%eax)
 	ret
-.size	aesni_decrypt,.-.L_aesni_decrypt_begin
+.size	_aesni_decrypt,.-.L_aesni_decrypt_begin
 .type	_aesni_encrypt3,@function
 .align	16
 _aesni_encrypt3:
@@ -335,10 +335,10 @@ _aesni_decrypt6:
 .byte	102,15,56,223,248
 	ret
 .size	_aesni_decrypt6,.-_aesni_decrypt6
-.globl	aesni_ecb_encrypt
-.type	aesni_ecb_encrypt,@function
+.globl	_aesni_ecb_encrypt
+.type	_aesni_ecb_encrypt,@function
 .align	16
-aesni_ecb_encrypt:
+_aesni_ecb_encrypt:
 .L_aesni_ecb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -563,11 +563,11 @@ aesni_ecb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_ecb_encrypt,.-.L_aesni_ecb_encrypt_begin
-.globl	aesni_ccm64_encrypt_blocks
-.type	aesni_ccm64_encrypt_blocks,@function
+.size	_aesni_ecb_encrypt,.-.L_aesni_ecb_encrypt_begin
+.globl	_aesni_ccm64_encrypt_blocks
+.type	_aesni_ccm64_encrypt_blocks,@function
 .align	16
-aesni_ccm64_encrypt_blocks:
+_aesni_ccm64_encrypt_blocks:
 .L_aesni_ccm64_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -643,11 +643,11 @@ aesni_ccm64_encrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_ccm64_encrypt_blocks,.-.L_aesni_ccm64_encrypt_blocks_begin
-.globl	aesni_ccm64_decrypt_blocks
-.type	aesni_ccm64_decrypt_blocks,@function
+.size	_aesni_ccm64_encrypt_blocks,.-.L_aesni_ccm64_encrypt_blocks_begin
+.globl	_aesni_ccm64_decrypt_blocks
+.type	_aesni_ccm64_decrypt_blocks,@function
 .align	16
-aesni_ccm64_decrypt_blocks:
+_aesni_ccm64_decrypt_blocks:
 .L_aesni_ccm64_decrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -755,11 +755,11 @@ aesni_ccm64_decrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_ccm64_decrypt_blocks,.-.L_aesni_ccm64_decrypt_blocks_begin
-.globl	aesni_ctr32_encrypt_blocks
-.type	aesni_ctr32_encrypt_blocks,@function
+.size	_aesni_ccm64_decrypt_blocks,.-.L_aesni_ccm64_decrypt_blocks_begin
+.globl	_aesni_ctr32_encrypt_blocks
+.type	_aesni_ctr32_encrypt_blocks,@function
 .align	16
-aesni_ctr32_encrypt_blocks:
+_aesni_ctr32_encrypt_blocks:
 .L_aesni_ctr32_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -986,11 +986,11 @@ aesni_ctr32_encrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_ctr32_encrypt_blocks,.-.L_aesni_ctr32_encrypt_blocks_begin
-.globl	aesni_xts_encrypt
-.type	aesni_xts_encrypt,@function
+.size	_aesni_ctr32_encrypt_blocks,.-.L_aesni_ctr32_encrypt_blocks_begin
+.globl	_aesni_xts_encrypt
+.type	_aesni_xts_encrypt,@function
 .align	16
-aesni_xts_encrypt:
+_aesni_xts_encrypt:
 .L_aesni_xts_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1333,11 +1333,11 @@ aesni_xts_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_xts_encrypt,.-.L_aesni_xts_encrypt_begin
-.globl	aesni_xts_decrypt
-.type	aesni_xts_decrypt,@function
+.size	_aesni_xts_encrypt,.-.L_aesni_xts_encrypt_begin
+.globl	_aesni_xts_decrypt
+.type	_aesni_xts_decrypt,@function
 .align	16
-aesni_xts_decrypt:
+_aesni_xts_decrypt:
 .L_aesni_xts_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1709,11 +1709,11 @@ aesni_xts_decrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_xts_decrypt,.-.L_aesni_xts_decrypt_begin
-.globl	aesni_cbc_encrypt
-.type	aesni_cbc_encrypt,@function
+.size	_aesni_xts_decrypt,.-.L_aesni_xts_decrypt_begin
+.globl	_aesni_cbc_encrypt
+.type	_aesni_cbc_encrypt,@function
 .align	16
-aesni_cbc_encrypt:
+_aesni_cbc_encrypt:
 .L_aesni_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1947,10 +1947,10 @@ aesni_cbc_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_cbc_encrypt,.-.L_aesni_cbc_encrypt_begin
-.type	_aesni_set_encrypt_key,@function
+.size	_aesni_cbc_encrypt,.-.L_aesni_cbc_encrypt_begin
+.type	__aesni_set_encrypt_key,@function
 .align	16
-_aesni_set_encrypt_key:
+__aesni_set_encrypt_key:
 	testl	%eax,%eax
 	jz	.L086bad_pointer
 	testl	%edx,%edx
@@ -2126,22 +2126,22 @@ _aesni_set_encrypt_key:
 .L089bad_keybits:
 	movl	$-2,%eax
 	ret
-.size	_aesni_set_encrypt_key,.-_aesni_set_encrypt_key
-.globl	aesni_set_encrypt_key
-.type	aesni_set_encrypt_key,@function
+.size	__aesni_set_encrypt_key,.-__aesni_set_encrypt_key
+.globl	_aesni_set_encrypt_key
+.type	_aesni_set_encrypt_key,@function
 .align	16
-aesni_set_encrypt_key:
+_aesni_set_encrypt_key:
 .L_aesni_set_encrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
 	movl	12(%esp),%edx
 	call	_aesni_set_encrypt_key
 	ret
-.size	aesni_set_encrypt_key,.-.L_aesni_set_encrypt_key_begin
-.globl	aesni_set_decrypt_key
-.type	aesni_set_decrypt_key,@function
+.size	_aesni_set_encrypt_key,.-.L_aesni_set_encrypt_key_begin
+.globl	_aesni_set_decrypt_key
+.type	_aesni_set_decrypt_key,@function
 .align	16
-aesni_set_decrypt_key:
+_aesni_set_decrypt_key:
 .L_aesni_set_decrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
@@ -2175,7 +2175,7 @@ aesni_set_decrypt_key:
 	xorl	%eax,%eax
 .L100dec_key_ret:
 	ret
-.size	aesni_set_decrypt_key,.-.L_aesni_set_decrypt_key_begin
+.size	_aesni_set_decrypt_key,.-.L_aesni_set_decrypt_key_begin
 .byte	65,69,83,32,102,111,114,32,73,110,116,101,108,32,65,69
 .byte	83,45,78,73,44,32,67,82,89,80,84,79,71,65,77,83
 .byte	32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115
diff --git a/lib/accelerated/x86/elf/aesni-x86_64.s b/lib/accelerated/x86/elf/aesni-x86_64.s
index 76d44fc..458d7fc 100644
--- a/lib/accelerated/x86/elf/aesni-x86_64.s
+++ b/lib/accelerated/x86/elf/aesni-x86_64.s
@@ -39,10 +39,10 @@
 #
 .text	
 
-.globl	aesni_encrypt
-.type	aesni_encrypt,@function
+.globl	_aesni_encrypt
+.type	_aesni_encrypt,@function
 .align	16
-aesni_encrypt:
+_aesni_encrypt:
 	movups	(%rdi),%xmm2
 	movl	240(%rdx),%eax
 	movups	(%rdx),%xmm0
@@ -61,12 +61,12 @@ aesni_encrypt:
 	movups	%xmm2,(%rsi)
 	pxor	%xmm2,%xmm2
 	.byte	0xf3,0xc3
-.size	aesni_encrypt,.-aesni_encrypt
+.size	_aesni_encrypt,.-_aesni_encrypt
 
-.globl	aesni_decrypt
-.type	aesni_decrypt,@function
+.globl	_aesni_decrypt
+.type	_aesni_decrypt,@function
 .align	16
-aesni_decrypt:
+_aesni_decrypt:
 	movups	(%rdi),%xmm2
 	movl	240(%rdx),%eax
 	movups	(%rdx),%xmm0
@@ -85,7 +85,7 @@ aesni_decrypt:
 	movups	%xmm2,(%rsi)
 	pxor	%xmm2,%xmm2
 	.byte	0xf3,0xc3
-.size	aesni_decrypt, .-aesni_decrypt
+.size	_aesni_decrypt, .-_aesni_decrypt
 .type	_aesni_encrypt2,@function
 .align	16
 _aesni_encrypt2:
@@ -528,10 +528,10 @@ _aesni_decrypt8:
 .byte	102,68,15,56,223,200
 	.byte	0xf3,0xc3
 .size	_aesni_decrypt8,.-_aesni_decrypt8
-.globl	aesni_ecb_encrypt
-.type	aesni_ecb_encrypt,@function
+.globl	_aesni_ecb_encrypt
+.type	_aesni_ecb_encrypt,@function
 .align	16
-aesni_ecb_encrypt:
+_aesni_ecb_encrypt:
 	andq	$-16,%rdx
 	jz	.Lecb_ret
 
@@ -869,11 +869,11 @@ aesni_ecb_encrypt:
 	xorps	%xmm0,%xmm0
 	pxor	%xmm1,%xmm1
 	.byte	0xf3,0xc3
-.size	aesni_ecb_encrypt,.-aesni_ecb_encrypt
-.globl	aesni_ccm64_encrypt_blocks
-.type	aesni_ccm64_encrypt_blocks,@function
+.size	_aesni_ecb_encrypt,.-_aesni_ecb_encrypt
+.globl	_aesni_ccm64_encrypt_blocks
+.type	_aesni_ccm64_encrypt_blocks,@function
 .align	16
-aesni_ccm64_encrypt_blocks:
+_aesni_ccm64_encrypt_blocks:
 	movl	240(%rcx),%eax
 	movdqu	(%r8),%xmm6
 	movdqa	.Lincrement64(%rip),%xmm9
@@ -932,11 +932,11 @@ aesni_ccm64_encrypt_blocks:
 	pxor	%xmm8,%xmm8
 	pxor	%xmm6,%xmm6
 	.byte	0xf3,0xc3
-.size	aesni_ccm64_encrypt_blocks,.-aesni_ccm64_encrypt_blocks
-.globl	aesni_ccm64_decrypt_blocks
-.type	aesni_ccm64_decrypt_blocks,@function
+.size	_aesni_ccm64_encrypt_blocks,.-_aesni_ccm64_encrypt_blocks
+.globl	_aesni_ccm64_decrypt_blocks
+.type	_aesni_ccm64_decrypt_blocks,@function
 .align	16
-aesni_ccm64_decrypt_blocks:
+_aesni_ccm64_decrypt_blocks:
 	movl	240(%rcx),%eax
 	movups	(%r8),%xmm6
 	movdqu	(%r9),%xmm3
@@ -1029,11 +1029,11 @@ aesni_ccm64_decrypt_blocks:
 	pxor	%xmm8,%xmm8
 	pxor	%xmm6,%xmm6
 	.byte	0xf3,0xc3
-.size	aesni_ccm64_decrypt_blocks,.-aesni_ccm64_decrypt_blocks
-.globl	aesni_ctr32_encrypt_blocks
-.type	aesni_ctr32_encrypt_blocks,@function
+.size	_aesni_ccm64_decrypt_blocks,.-_aesni_ccm64_decrypt_blocks
+.globl	_aesni_ctr32_encrypt_blocks
+.type	_aesni_ctr32_encrypt_blocks,@function
 .align	16
-aesni_ctr32_encrypt_blocks:
+_aesni_ctr32_encrypt_blocks:
 	cmpq	$1,%rdx
 	jne	.Lctr32_bulk
 
@@ -1602,11 +1602,11 @@ aesni_ctr32_encrypt_blocks:
 	popq	%rbp
 .Lctr32_epilogue:
 	.byte	0xf3,0xc3
-.size	aesni_ctr32_encrypt_blocks,.-aesni_ctr32_encrypt_blocks
-.globl	aesni_xts_encrypt
-.type	aesni_xts_encrypt,@function
+.size	_aesni_ctr32_encrypt_blocks,.-_aesni_ctr32_encrypt_blocks
+.globl	_aesni_xts_encrypt
+.type	_aesni_xts_encrypt,@function
 .align	16
-aesni_xts_encrypt:
+_aesni_xts_encrypt:
 	leaq	(%rsp),%rax
 	pushq	%rbp
 	subq	$112,%rsp
@@ -2067,11 +2067,11 @@ aesni_xts_encrypt:
 	popq	%rbp
 .Lxts_enc_epilogue:
 	.byte	0xf3,0xc3
-.size	aesni_xts_encrypt,.-aesni_xts_encrypt
-.globl	aesni_xts_decrypt
-.type	aesni_xts_decrypt,@function
+.size	_aesni_xts_encrypt,.-_aesni_xts_encrypt
+.globl	_aesni_xts_decrypt
+.type	_aesni_xts_decrypt,@function
 .align	16
-aesni_xts_decrypt:
+_aesni_xts_decrypt:
 	leaq	(%rsp),%rax
 	pushq	%rbp
 	subq	$112,%rsp
@@ -2569,11 +2569,11 @@ aesni_xts_decrypt:
 	popq	%rbp
 .Lxts_dec_epilogue:
 	.byte	0xf3,0xc3
-.size	aesni_xts_decrypt,.-aesni_xts_decrypt
-.globl	aesni_cbc_encrypt
-.type	aesni_cbc_encrypt,@function
+.size	_aesni_xts_decrypt,.-_aesni_xts_decrypt
+.globl	_aesni_cbc_encrypt
+.type	_aesni_cbc_encrypt,@function
 .align	16
-aesni_cbc_encrypt:
+_aesni_cbc_encrypt:
 	testq	%rdx,%rdx
 	jz	.Lcbc_ret
 
@@ -3154,11 +3154,11 @@ aesni_cbc_encrypt:
 	popq	%rbp
 .Lcbc_ret:
 	.byte	0xf3,0xc3
-.size	aesni_cbc_encrypt,.-aesni_cbc_encrypt
-.globl	aesni_set_decrypt_key
-.type	aesni_set_decrypt_key,@function
+.size	_aesni_cbc_encrypt,.-_aesni_cbc_encrypt
+.globl	_aesni_set_decrypt_key
+.type	_aesni_set_decrypt_key,@function
 .align	16
-aesni_set_decrypt_key:
+_aesni_set_decrypt_key:
 .byte	0x48,0x83,0xEC,0x08
 	call	__aesni_set_encrypt_key
 	shll	$4,%esi
@@ -3194,11 +3194,11 @@ aesni_set_decrypt_key:
 	addq	$8,%rsp
 	.byte	0xf3,0xc3
 .LSEH_end_set_decrypt_key:
-.size	aesni_set_decrypt_key,.-aesni_set_decrypt_key
-.globl	aesni_set_encrypt_key
-.type	aesni_set_encrypt_key,@function
+.size	_aesni_set_decrypt_key,.-_aesni_set_decrypt_key
+.globl	_aesni_set_encrypt_key
+.type	_aesni_set_encrypt_key,@function
 .align	16
-aesni_set_encrypt_key:
+_aesni_set_encrypt_key:
 __aesni_set_encrypt_key:
 .byte	0x48,0x83,0xEC,0x08
 	movq	$-1,%rax
@@ -3564,7 +3564,7 @@ __aesni_set_encrypt_key:
 	shufps	$170,%xmm1,%xmm1
 	xorps	%xmm1,%xmm2
 	.byte	0xf3,0xc3
-.size	aesni_set_encrypt_key,.-aesni_set_encrypt_key
+.size	_aesni_set_encrypt_key,.-_aesni_set_encrypt_key
 .size	__aesni_set_encrypt_key,.-__aesni_set_encrypt_key
 .align	64
 .Lbswap_mask:
diff --git a/lib/accelerated/x86/elf/sha1-ssse3-x86.s b/lib/accelerated/x86/elf/sha1-ssse3-x86.s
index 7b585a0..888e94a 100644
--- a/lib/accelerated/x86/elf/sha1-ssse3-x86.s
+++ b/lib/accelerated/x86/elf/sha1-ssse3-x86.s
@@ -39,10 +39,10 @@
 #
 .file	"sha1-586.s"
 .text
-.globl	sha1_block_data_order
-.type	sha1_block_data_order,@function
+.globl	_sha1_block_data_order
+.type	_sha1_block_data_order,@function
 .align	16
-sha1_block_data_order:
+_sha1_block_data_order:
 .L_sha1_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1412,7 +1412,7 @@ sha1_block_data_order:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	sha1_block_data_order,.-.L_sha1_block_data_order_begin
+.size	_sha1_block_data_order,.-.L_sha1_block_data_order_begin
 .byte	83,72,65,49,32,98,108,111,99,107,32,116,114,97,110,115
 .byte	102,111,114,109,32,102,111,114,32,120,56,54,44,32,67,82
 .byte	89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112
diff --git a/lib/accelerated/x86/elf/sha1-ssse3-x86_64.s b/lib/accelerated/x86/elf/sha1-ssse3-x86_64.s
index af40532..1523e2a 100644
--- a/lib/accelerated/x86/elf/sha1-ssse3-x86_64.s
+++ b/lib/accelerated/x86/elf/sha1-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	sha1_block_data_order
-.type	sha1_block_data_order,@function
+.globl	_sha1_block_data_order
+.type	_sha1_block_data_order,@function
 .align	16
-sha1_block_data_order:
+_sha1_block_data_order:
 	movl	_gnutls_x86_cpuid_s+0(%rip),%r9d
 	movl	_gnutls_x86_cpuid_s+4(%rip),%r8d
 	testl	$512,%r8d
@@ -1326,10 +1326,10 @@ sha1_block_data_order:
 	leaq	32(%rsi),%rsp
 .Lepilogue:
 	.byte	0xf3,0xc3
-.size	sha1_block_data_order,.-sha1_block_data_order
-.type	sha1_block_data_order_ssse3,@function
+.size	_sha1_block_data_order,.-_sha1_block_data_order
+.type	_sha1_block_data_order_ssse3,@function
 .align	16
-sha1_block_data_order_ssse3:
+_sha1_block_data_order_ssse3:
 _ssse3_shortcut:
 	pushq	%rbx
 	pushq	%rbp
@@ -2495,7 +2495,7 @@ _ssse3_shortcut:
 	leaq	24(%rsi),%rsp
 .Lepilogue_ssse3:
 	.byte	0xf3,0xc3
-.size	sha1_block_data_order_ssse3,.-sha1_block_data_order_ssse3
+.size	_sha1_block_data_order_ssse3,.-_sha1_block_data_order_ssse3
 .align	64
 K_XX_XX:
 .long	0x5a827999,0x5a827999,0x5a827999,0x5a827999	
diff --git a/lib/accelerated/x86/elf/sha256-ssse3-x86.s b/lib/accelerated/x86/elf/sha256-ssse3-x86.s
index 7470ef7..e6eca12 100644
--- a/lib/accelerated/x86/elf/sha256-ssse3-x86.s
+++ b/lib/accelerated/x86/elf/sha256-ssse3-x86.s
@@ -39,10 +39,10 @@
 #
 .file	"sha512-586.s"
 .text
-.globl	sha256_block_data_order
-.type	sha256_block_data_order,@function
+.globl	_sha256_block_data_order
+.type	_sha256_block_data_order,@function
 .align	16
-sha256_block_data_order:
+_sha256_block_data_order:
 .L_sha256_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -3392,7 +3392,7 @@ sha256_block_data_order:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	sha256_block_data_order,.-.L_sha256_block_data_order_begin
+.size	_sha256_block_data_order,.-.L_sha256_block_data_order_begin
 .byte	83,72,65,50,53,54,32,98,108,111,99,107,32,116,114,97
 .byte	110,115,102,111,114,109,32,102,111,114,32,120,56,54,44,32
 .byte	67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97
diff --git a/lib/accelerated/x86/elf/sha512-ssse3-x86.s b/lib/accelerated/x86/elf/sha512-ssse3-x86.s
index 0b99b22..67c1ff5 100644
--- a/lib/accelerated/x86/elf/sha512-ssse3-x86.s
+++ b/lib/accelerated/x86/elf/sha512-ssse3-x86.s
@@ -39,10 +39,10 @@
 #
 .file	"sha512-586.s"
 .text
-.globl	sha512_block_data_order
-.type	sha512_block_data_order,@function
+.globl	_sha512_block_data_order
+.type	_sha512_block_data_order,@function
 .align	16
-sha512_block_data_order:
+_sha512_block_data_order:
 .L_sha512_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -594,7 +594,7 @@ sha512_block_data_order:
 .long	4234509866,1501505948
 .long	987167468,1607167915
 .long	1246189591,1816402316
-.size	sha512_block_data_order,.-.L_sha512_block_data_order_begin
+.size	_sha512_block_data_order,.-.L_sha512_block_data_order_begin
 .byte	83,72,65,53,49,50,32,98,108,111,99,107,32,116,114,97
 .byte	110,115,102,111,114,109,32,102,111,114,32,120,56,54,44,32
 .byte	67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97
diff --git a/lib/accelerated/x86/elf/sha512-ssse3-x86_64.s b/lib/accelerated/x86/elf/sha512-ssse3-x86_64.s
index d51d816..f2a9b57 100644
--- a/lib/accelerated/x86/elf/sha512-ssse3-x86_64.s
+++ b/lib/accelerated/x86/elf/sha512-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	sha256_block_data_order
-.type	sha256_block_data_order,@function
+.globl	_sha256_block_data_order
+.type	_sha256_block_data_order,@function
 .align	16
-sha256_block_data_order:
+_sha256_block_data_order:
 	leaq	_gnutls_x86_cpuid_s(%rip),%r11
 	movl	0(%r11),%r9d
 	movl	4(%r11),%r10d
@@ -1755,7 +1755,7 @@ sha256_block_data_order:
 	leaq	48(%rsi),%rsp
 .Lepilogue:
 	.byte	0xf3,0xc3
-.size	sha256_block_data_order,.-sha256_block_data_order
+.size	_sha256_block_data_order,.-_sha256_block_data_order
 .align	64
 .type	K256,@object
 K256:
@@ -1799,9 +1799,9 @@ K256:
 .long	0xffffffff,0xffffffff,0x03020100,0x0b0a0908
 .long	0xffffffff,0xffffffff,0x03020100,0x0b0a0908
 .byte	83,72,65,50,53,54,32,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
-.type	sha256_block_data_order_ssse3,@function
+.type	_sha256_block_data_order_ssse3,@function
 .align	64
-sha256_block_data_order_ssse3:
+_sha256_block_data_order_ssse3:
 .Lssse3_shortcut:
 	pushq	%rbx
 	pushq	%rbp
@@ -2885,7 +2885,7 @@ sha256_block_data_order_ssse3:
 	leaq	48(%rsi),%rsp
 .Lepilogue_ssse3:
 	.byte	0xf3,0xc3
-.size	sha256_block_data_order_ssse3,.-sha256_block_data_order_ssse3
+.size	_sha256_block_data_order_ssse3,.-_sha256_block_data_order_ssse3
 
 
 .section .note.GNU-stack,"",%progbits
diff --git a/lib/accelerated/x86/macosx/aesni-x86.s b/lib/accelerated/x86/macosx/aesni-x86.s
index 09ca1cb..d9bde6e 100644
--- a/lib/accelerated/x86/macosx/aesni-x86.s
+++ b/lib/accelerated/x86/macosx/aesni-x86.s
@@ -39,10 +39,10 @@
 #
 .file	"devel/perlasm/aesni-x86.s"
 .text
-.globl	_aesni_encrypt
+.globl	__aesni_encrypt
 .align	4
-_aesni_encrypt:
-L_aesni_encrypt_begin:
+__aesni_encrypt:
+L__aesni_encrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
 	movups	(%eax),%xmm2
@@ -61,10 +61,10 @@ L000enc1_loop_1:
 .byte	102,15,56,221,209
 	movups	%xmm2,(%eax)
 	ret
-.globl	_aesni_decrypt
+.globl	__aesni_decrypt
 .align	4
-_aesni_decrypt:
-L_aesni_decrypt_begin:
+__aesni_decrypt:
+L__aesni_decrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
 	movups	(%eax),%xmm2
@@ -84,7 +84,7 @@ L001dec1_loop_2:
 	movups	%xmm2,(%eax)
 	ret
 .align	4
-__aesni_encrypt3:
+___aesni_encrypt3:
 	movups	(%edx),%xmm0
 	shrl	$1,%ecx
 	movups	16(%edx),%xmm1
@@ -113,7 +113,7 @@ L002enc3_loop:
 .byte	102,15,56,221,224
 	ret
 .align	4
-__aesni_decrypt3:
+___aesni_decrypt3:
 	movups	(%edx),%xmm0
 	shrl	$1,%ecx
 	movups	16(%edx),%xmm1
@@ -142,7 +142,7 @@ L003dec3_loop:
 .byte	102,15,56,223,224
 	ret
 .align	4
-__aesni_encrypt4:
+___aesni_encrypt4:
 	movups	(%edx),%xmm0
 	movups	16(%edx),%xmm1
 	shrl	$1,%ecx
@@ -176,7 +176,7 @@ L004enc4_loop:
 .byte	102,15,56,221,232
 	ret
 .align	4
-__aesni_decrypt4:
+___aesni_decrypt4:
 	movups	(%edx),%xmm0
 	movups	16(%edx),%xmm1
 	shrl	$1,%ecx
@@ -210,7 +210,7 @@ L005dec4_loop:
 .byte	102,15,56,223,232
 	ret
 .align	4
-__aesni_encrypt6:
+___aesni_encrypt6:
 	movups	(%edx),%xmm0
 	shrl	$1,%ecx
 	movups	16(%edx),%xmm1
@@ -229,7 +229,7 @@ __aesni_encrypt6:
 .byte	102,15,56,220,241
 	movups	(%edx),%xmm0
 .byte	102,15,56,220,249
-	jmp	L_aesni_encrypt6_enter
+	jmp	L__aesni_encrypt6_enter
 .align	4,0x90
 L006enc6_loop:
 .byte	102,15,56,220,209
@@ -240,7 +240,7 @@ L006enc6_loop:
 .byte	102,15,56,220,241
 .byte	102,15,56,220,249
 .align	4,0x90
-L_aesni_encrypt6_enter:
+L__aesni_encrypt6_enter:
 	movups	16(%edx),%xmm1
 .byte	102,15,56,220,208
 .byte	102,15,56,220,216
@@ -265,7 +265,7 @@ L_aesni_encrypt6_enter:
 .byte	102,15,56,221,248
 	ret
 .align	4
-__aesni_decrypt6:
+___aesni_decrypt6:
 	movups	(%edx),%xmm0
 	shrl	$1,%ecx
 	movups	16(%edx),%xmm1
@@ -284,7 +284,7 @@ __aesni_decrypt6:
 .byte	102,15,56,222,241
 	movups	(%edx),%xmm0
 .byte	102,15,56,222,249
-	jmp	L_aesni_decrypt6_enter
+	jmp	L__aesni_decrypt6_enter
 .align	4,0x90
 L007dec6_loop:
 .byte	102,15,56,222,209
@@ -295,7 +295,7 @@ L007dec6_loop:
 .byte	102,15,56,222,241
 .byte	102,15,56,222,249
 .align	4,0x90
-L_aesni_decrypt6_enter:
+L__aesni_decrypt6_enter:
 	movups	16(%edx),%xmm1
 .byte	102,15,56,222,208
 .byte	102,15,56,222,216
@@ -319,10 +319,10 @@ L_aesni_decrypt6_enter:
 .byte	102,15,56,223,240
 .byte	102,15,56,223,248
 	ret
-.globl	_aesni_ecb_encrypt
+.globl	__aesni_ecb_encrypt
 .align	4
-_aesni_ecb_encrypt:
-L_aesni_ecb_encrypt_begin:
+__aesni_ecb_encrypt:
+L__aesni_ecb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -367,7 +367,7 @@ L012ecb_enc_loop6:
 	movdqu	80(%esi),%xmm7
 	leal	96(%esi),%esi
 L011ecb_enc_loop6_enter:
-	call	__aesni_encrypt6
+	call	___aesni_encrypt6
 	movl	%ebp,%edx
 	movl	%ebx,%ecx
 	subl	$96,%eax
@@ -394,7 +394,7 @@ L010ecb_enc_tail:
 	je	L016ecb_enc_four
 	movups	64(%esi),%xmm6
 	xorps	%xmm7,%xmm7
-	call	__aesni_encrypt6
+	call	___aesni_encrypt6
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
@@ -419,20 +419,20 @@ L017enc1_loop_3:
 .align	4,0x90
 L014ecb_enc_two:
 	xorps	%xmm4,%xmm4
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	jmp	L008ecb_ret
 .align	4,0x90
 L015ecb_enc_three:
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
 	jmp	L008ecb_ret
 .align	4,0x90
 L016ecb_enc_four:
-	call	__aesni_encrypt4
+	call	___aesni_encrypt4
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
@@ -470,7 +470,7 @@ L020ecb_dec_loop6:
 	movdqu	80(%esi),%xmm7
 	leal	96(%esi),%esi
 L019ecb_dec_loop6_enter:
-	call	__aesni_decrypt6
+	call	___aesni_decrypt6
 	movl	%ebp,%edx
 	movl	%ebx,%ecx
 	subl	$96,%eax
@@ -497,7 +497,7 @@ L018ecb_dec_tail:
 	je	L024ecb_dec_four
 	movups	64(%esi),%xmm6
 	xorps	%xmm7,%xmm7
-	call	__aesni_decrypt6
+	call	___aesni_decrypt6
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
@@ -522,20 +522,20 @@ L025dec1_loop_4:
 .align	4,0x90
 L022ecb_dec_two:
 	xorps	%xmm4,%xmm4
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	jmp	L008ecb_ret
 .align	4,0x90
 L023ecb_dec_three:
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
 	jmp	L008ecb_ret
 .align	4,0x90
 L024ecb_dec_four:
-	call	__aesni_decrypt4
+	call	___aesni_decrypt4
 	movups	%xmm2,(%edi)
 	movups	%xmm3,16(%edi)
 	movups	%xmm4,32(%edi)
@@ -546,10 +546,10 @@ L008ecb_ret:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ccm64_encrypt_blocks
+.globl	__aesni_ccm64_encrypt_blocks
 .align	4
-_aesni_ccm64_encrypt_blocks:
-L_aesni_ccm64_encrypt_blocks_begin:
+__aesni_ccm64_encrypt_blocks:
+L__aesni_ccm64_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -624,10 +624,10 @@ L027ccm64_enc2_loop:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ccm64_decrypt_blocks
+.globl	__aesni_ccm64_decrypt_blocks
 .align	4
-_aesni_ccm64_decrypt_blocks:
-L_aesni_ccm64_decrypt_blocks_begin:
+__aesni_ccm64_decrypt_blocks:
+L__aesni_ccm64_decrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -734,10 +734,10 @@ L032enc1_loop_6:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ctr32_encrypt_blocks
+.globl	__aesni_ctr32_encrypt_blocks
 .align	4
-_aesni_ctr32_encrypt_blocks:
-L_aesni_ctr32_encrypt_blocks_begin:
+__aesni_ctr32_encrypt_blocks:
+L__aesni_ctr32_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -826,7 +826,7 @@ L035ctr32_loop6:
 .byte	102,15,56,220,241
 	movups	(%edx),%xmm0
 .byte	102,15,56,220,249
-	call	L_aesni_encrypt6_enter
+	call	L__aesni_encrypt6_enter
 	movups	(%esi),%xmm1
 	movups	16(%esi),%xmm0
 	xorps	%xmm1,%xmm2
@@ -881,7 +881,7 @@ L034ctr32_tail:
 	por	%xmm7,%xmm5
 	je	L040ctr32_four
 	por	%xmm7,%xmm6
-	call	__aesni_encrypt6
+	call	___aesni_encrypt6
 	movups	(%esi),%xmm1
 	movups	16(%esi),%xmm0
 	xorps	%xmm1,%xmm2
@@ -920,7 +920,7 @@ L041enc1_loop_7:
 	jmp	L036ctr32_ret
 .align	4,0x90
 L038ctr32_two:
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	movups	(%esi),%xmm5
 	movups	16(%esi),%xmm6
 	xorps	%xmm5,%xmm2
@@ -930,7 +930,7 @@ L038ctr32_two:
 	jmp	L036ctr32_ret
 .align	4,0x90
 L039ctr32_three:
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	movups	(%esi),%xmm5
 	movups	16(%esi),%xmm6
 	xorps	%xmm5,%xmm2
@@ -943,7 +943,7 @@ L039ctr32_three:
 	jmp	L036ctr32_ret
 .align	4,0x90
 L040ctr32_four:
-	call	__aesni_encrypt4
+	call	___aesni_encrypt4
 	movups	(%esi),%xmm6
 	movups	16(%esi),%xmm7
 	movups	32(%esi),%xmm1
@@ -963,10 +963,10 @@ L036ctr32_ret:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_xts_encrypt
+.globl	__aesni_xts_encrypt
 .align	4
-_aesni_xts_encrypt:
-L_aesni_xts_encrypt_begin:
+__aesni_xts_encrypt:
+L__aesni_xts_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -1078,7 +1078,7 @@ L044xts_enc_loop6:
 .byte	102,15,56,220,241
 	movups	(%edx),%xmm0
 .byte	102,15,56,220,249
-	call	L_aesni_encrypt6_enter
+	call	L__aesni_encrypt6_enter
 	movdqa	80(%esp),%xmm1
 	pxor	%xmm0,%xmm0
 	xorps	(%esp),%xmm2
@@ -1158,7 +1158,7 @@ L043xts_enc_short:
 	pxor	48(%esp),%xmm5
 	movdqa	%xmm7,64(%esp)
 	pxor	%xmm7,%xmm6
-	call	__aesni_encrypt6
+	call	___aesni_encrypt6
 	movaps	64(%esp),%xmm1
 	xorps	(%esp),%xmm2
 	xorps	16(%esp),%xmm3
@@ -1202,7 +1202,7 @@ L047xts_enc_two:
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm4,%xmm4
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	movups	%xmm2,(%edi)
@@ -1220,7 +1220,7 @@ L048xts_enc_three:
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm7,%xmm4
-	call	__aesni_encrypt3
+	call	___aesni_encrypt3
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm7,%xmm4
@@ -1242,7 +1242,7 @@ L049xts_enc_four:
 	xorps	16(%esp),%xmm3
 	xorps	%xmm7,%xmm4
 	xorps	%xmm6,%xmm5
-	call	__aesni_encrypt4
+	call	___aesni_encrypt4
 	xorps	(%esp),%xmm2
 	xorps	16(%esp),%xmm3
 	xorps	%xmm7,%xmm4
@@ -1308,10 +1308,10 @@ L052xts_enc_ret:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_xts_decrypt
+.globl	__aesni_xts_decrypt
 .align	4
-_aesni_xts_decrypt:
-L_aesni_xts_decrypt_begin:
+__aesni_xts_decrypt:
+L__aesni_xts_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -1428,7 +1428,7 @@ L057xts_dec_loop6:
 .byte	102,15,56,222,241
 	movups	(%edx),%xmm0
 .byte	102,15,56,222,249
-	call	L_aesni_decrypt6_enter
+	call	L__aesni_decrypt6_enter
 	movdqa	80(%esp),%xmm1
 	pxor	%xmm0,%xmm0
 	xorps	(%esp),%xmm2
@@ -1508,7 +1508,7 @@ L056xts_dec_short:
 	pxor	48(%esp),%xmm5
 	movdqa	%xmm7,64(%esp)
 	pxor	%xmm7,%xmm6
-	call	__aesni_decrypt6
+	call	___aesni_decrypt6
 	movaps	64(%esp),%xmm1
 	xorps	(%esp),%xmm2
 	xorps	16(%esp),%xmm3
@@ -1551,7 +1551,7 @@ L060xts_dec_two:
 	leal	32(%esi),%esi
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	movups	%xmm2,(%edi)
@@ -1569,7 +1569,7 @@ L061xts_dec_three:
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm7,%xmm4
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	xorps	%xmm5,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm7,%xmm4
@@ -1591,7 +1591,7 @@ L062xts_dec_four:
 	xorps	16(%esp),%xmm3
 	xorps	%xmm7,%xmm4
 	xorps	%xmm6,%xmm5
-	call	__aesni_decrypt4
+	call	___aesni_decrypt4
 	xorps	(%esp),%xmm2
 	xorps	16(%esp),%xmm3
 	xorps	%xmm7,%xmm4
@@ -1682,10 +1682,10 @@ L065xts_dec_ret:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_cbc_encrypt
+.globl	__aesni_cbc_encrypt
 .align	4
-_aesni_cbc_encrypt:
-L_aesni_cbc_encrypt_begin:
+__aesni_cbc_encrypt:
+L__aesni_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
@@ -1770,7 +1770,7 @@ L077cbc_dec_loop6_enter:
 	movdqu	48(%esi),%xmm5
 	movdqu	64(%esi),%xmm6
 	movdqu	80(%esi),%xmm7
-	call	__aesni_decrypt6
+	call	___aesni_decrypt6
 	movups	(%esi),%xmm1
 	movups	16(%esi),%xmm0
 	xorps	(%esp),%xmm2
@@ -1819,7 +1819,7 @@ L076cbc_dec_tail:
 	movaps	%xmm7,(%esp)
 	movups	(%esi),%xmm2
 	xorps	%xmm7,%xmm7
-	call	__aesni_decrypt6
+	call	___aesni_decrypt6
 	movups	(%esi),%xmm1
 	movups	16(%esi),%xmm0
 	xorps	(%esp),%xmm2
@@ -1858,7 +1858,7 @@ L084dec1_loop_16:
 .align	4,0x90
 L081cbc_dec_two:
 	xorps	%xmm4,%xmm4
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	xorps	%xmm7,%xmm2
 	xorps	%xmm6,%xmm3
 	movups	%xmm2,(%edi)
@@ -1869,7 +1869,7 @@ L081cbc_dec_two:
 	jmp	L079cbc_dec_tail_collected
 .align	4,0x90
 L082cbc_dec_three:
-	call	__aesni_decrypt3
+	call	___aesni_decrypt3
 	xorps	%xmm7,%xmm2
 	xorps	%xmm6,%xmm3
 	xorps	%xmm5,%xmm4
@@ -1882,7 +1882,7 @@ L082cbc_dec_three:
 	jmp	L079cbc_dec_tail_collected
 .align	4,0x90
 L083cbc_dec_four:
-	call	__aesni_decrypt4
+	call	___aesni_decrypt4
 	movups	16(%esi),%xmm1
 	movups	32(%esi),%xmm0
 	xorps	%xmm7,%xmm2
@@ -1919,7 +1919,7 @@ L070cbc_abort:
 	popl	%ebp
 	ret
 .align	4
-__aesni_set_encrypt_key:
+___aesni_set_encrypt_key:
 	testl	%eax,%eax
 	jz	L086bad_pointer
 	testl	%edx,%edx
@@ -2095,23 +2095,23 @@ L086bad_pointer:
 L089bad_keybits:
 	movl	$-2,%eax
 	ret
-.globl	_aesni_set_encrypt_key
+.globl	__aesni_set_encrypt_key
 .align	4
-_aesni_set_encrypt_key:
-L_aesni_set_encrypt_key_begin:
+__aesni_set_encrypt_key:
+L__aesni_set_encrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
 	movl	12(%esp),%edx
-	call	__aesni_set_encrypt_key
+	call	___aesni_set_encrypt_key
 	ret
-.globl	_aesni_set_decrypt_key
+.globl	__aesni_set_decrypt_key
 .align	4
-_aesni_set_decrypt_key:
-L_aesni_set_decrypt_key_begin:
+__aesni_set_decrypt_key:
+L__aesni_set_decrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
 	movl	12(%esp),%edx
-	call	__aesni_set_encrypt_key
+	call	___aesni_set_encrypt_key
 	movl	12(%esp),%edx
 	shll	$4,%ecx
 	testl	%eax,%eax
diff --git a/lib/accelerated/x86/macosx/aesni-x86_64.s b/lib/accelerated/x86/macosx/aesni-x86_64.s
index f0a5606..f66a101 100644
--- a/lib/accelerated/x86/macosx/aesni-x86_64.s
+++ b/lib/accelerated/x86/macosx/aesni-x86_64.s
@@ -39,10 +39,10 @@
 #
 .text	
 
-.globl	_aesni_encrypt
+.globl	__aesni_encrypt
 
 .p2align	4
-_aesni_encrypt:
+__aesni_encrypt:
 	movups	(%rdi),%xmm2
 	movl	240(%rdx),%eax
 	movups	(%rdx),%xmm0
@@ -63,10 +63,10 @@ L$oop_enc1_1:
 	.byte	0xf3,0xc3
 
 
-.globl	_aesni_decrypt
+.globl	__aesni_decrypt
 
 .p2align	4
-_aesni_decrypt:
+__aesni_decrypt:
 	movups	(%rdi),%xmm2
 	movl	240(%rdx),%eax
 	movups	(%rdx),%xmm0
@@ -88,7 +88,7 @@ L$oop_dec1_2:
 
 
 .p2align	4
-_aesni_encrypt2:
+__aesni_encrypt2:
 	movups	(%rcx),%xmm0
 	shll	$4,%eax
 	movups	16(%rcx),%xmm1
@@ -117,7 +117,7 @@ L$enc_loop2:
 
 
 .p2align	4
-_aesni_decrypt2:
+__aesni_decrypt2:
 	movups	(%rcx),%xmm0
 	shll	$4,%eax
 	movups	16(%rcx),%xmm1
@@ -146,7 +146,7 @@ L$dec_loop2:
 
 
 .p2align	4
-_aesni_encrypt3:
+__aesni_encrypt3:
 	movups	(%rcx),%xmm0
 	shll	$4,%eax
 	movups	16(%rcx),%xmm1
@@ -180,7 +180,7 @@ L$enc_loop3:
 
 
 .p2align	4
-_aesni_decrypt3:
+__aesni_decrypt3:
 	movups	(%rcx),%xmm0
 	shll	$4,%eax
 	movups	16(%rcx),%xmm1
@@ -214,7 +214,7 @@ L$dec_loop3:
 
 
 .p2align	4
-_aesni_encrypt4:
+__aesni_encrypt4:
 	movups	(%rcx),%xmm0
 	shll	$4,%eax
 	movups	16(%rcx),%xmm1
@@ -254,7 +254,7 @@ L$enc_loop4:
 
 
 .p2align	4
-_aesni_decrypt4:
+__aesni_decrypt4:
 	movups	(%rcx),%xmm0
 	shll	$4,%eax
 	movups	16(%rcx),%xmm1
@@ -294,7 +294,7 @@ L$dec_loop4:
 
 
 .p2align	4
-_aesni_encrypt6:
+__aesni_encrypt6:
 	movups	(%rcx),%xmm0
 	shll	$4,%eax
 	movups	16(%rcx),%xmm1
@@ -348,7 +348,7 @@ L$enc_loop6_enter:
 
 
 .p2align	4
-_aesni_decrypt6:
+__aesni_decrypt6:
 	movups	(%rcx),%xmm0
 	shll	$4,%eax
 	movups	16(%rcx),%xmm1
@@ -402,7 +402,7 @@ L$dec_loop6_enter:
 
 
 .p2align	4
-_aesni_encrypt8:
+__aesni_encrypt8:
 	movups	(%rcx),%xmm0
 	shll	$4,%eax
 	movups	16(%rcx),%xmm1
@@ -466,7 +466,7 @@ L$enc_loop8_enter:
 
 
 .p2align	4
-_aesni_decrypt8:
+__aesni_decrypt8:
 	movups	(%rcx),%xmm0
 	shll	$4,%eax
 	movups	16(%rcx),%xmm1
@@ -528,10 +528,10 @@ L$dec_loop8_enter:
 .byte	102,68,15,56,223,200
 	.byte	0xf3,0xc3
 
-.globl	_aesni_ecb_encrypt
+.globl	__aesni_ecb_encrypt
 
 .p2align	4
-_aesni_ecb_encrypt:
+__aesni_ecb_encrypt:
 	andq	$-16,%rdx
 	jz	L$ecb_ret
 
@@ -580,7 +580,7 @@ L$ecb_enc_loop8:
 	leaq	128(%rdi),%rdi
 L$ecb_enc_loop8_enter:
 
-	call	_aesni_encrypt8
+	call	__aesni_encrypt8
 
 	subq	$0x80,%rdx
 	jnc	L$ecb_enc_loop8
@@ -617,7 +617,7 @@ L$ecb_enc_tail:
 	je	L$ecb_enc_six
 	movdqu	96(%rdi),%xmm8
 	xorps	%xmm9,%xmm9
-	call	_aesni_encrypt8
+	call	__aesni_encrypt8
 	movups	%xmm2,(%rsi)
 	movups	%xmm3,16(%rsi)
 	movups	%xmm4,32(%rsi)
@@ -643,20 +643,20 @@ L$oop_enc1_3:
 	jmp	L$ecb_ret
 .p2align	4
 L$ecb_enc_two:
-	call	_aesni_encrypt2
+	call	__aesni_encrypt2
 	movups	%xmm2,(%rsi)
 	movups	%xmm3,16(%rsi)
 	jmp	L$ecb_ret
 .p2align	4
 L$ecb_enc_three:
-	call	_aesni_encrypt3
+	call	__aesni_encrypt3
 	movups	%xmm2,(%rsi)
 	movups	%xmm3,16(%rsi)
 	movups	%xmm4,32(%rsi)
 	jmp	L$ecb_ret
 .p2align	4
 L$ecb_enc_four:
-	call	_aesni_encrypt4
+	call	__aesni_encrypt4
 	movups	%xmm2,(%rsi)
 	movups	%xmm3,16(%rsi)
 	movups	%xmm4,32(%rsi)
@@ -665,7 +665,7 @@ L$ecb_enc_four:
 .p2align	4
 L$ecb_enc_five:
 	xorps	%xmm7,%xmm7
-	call	_aesni_encrypt6
+	call	__aesni_encrypt6
 	movups	%xmm2,(%rsi)
 	movups	%xmm3,16(%rsi)
 	movups	%xmm4,32(%rsi)
@@ -674,7 +674,7 @@ L$ecb_enc_five:
 	jmp	L$ecb_ret
 .p2align	4
 L$ecb_enc_six:
-	call	_aesni_encrypt6
+	call	__aesni_encrypt6
 	movups	%xmm2,(%rsi)
 	movups	%xmm3,16(%rsi)
 	movups	%xmm4,32(%rsi)
@@ -723,7 +723,7 @@ L$ecb_dec_loop8:
 	leaq	128(%rdi),%rdi
 L$ecb_dec_loop8_enter:
 
-	call	_aesni_decrypt8
+	call	__aesni_decrypt8
 
 	movups	(%r11),%xmm0
 	subq	$0x80,%rdx
@@ -770,7 +770,7 @@ L$ecb_dec_tail:
 	movups	96(%rdi),%xmm8
 	movups	(%rcx),%xmm0
 	xorps	%xmm9,%xmm9
-	call	_aesni_decrypt8
+	call	__aesni_decrypt8
 	movups	%xmm2,(%rsi)
 	pxor	%xmm2,%xmm2
 	movups	%xmm3,16(%rsi)
@@ -805,7 +805,7 @@ L$oop_dec1_4:
 	jmp	L$ecb_ret
 .p2align	4
 L$ecb_dec_two:
-	call	_aesni_decrypt2
+	call	__aesni_decrypt2
 	movups	%xmm2,(%rsi)
 	pxor	%xmm2,%xmm2
 	movups	%xmm3,16(%rsi)
@@ -813,7 +813,7 @@ L$ecb_dec_two:
 	jmp	L$ecb_ret
 .p2align	4
 L$ecb_dec_three:
-	call	_aesni_decrypt3
+	call	__aesni_decrypt3
 	movups	%xmm2,(%rsi)
 	pxor	%xmm2,%xmm2
 	movups	%xmm3,16(%rsi)
@@ -823,7 +823,7 @@ L$ecb_dec_three:
 	jmp	L$ecb_ret
 .p2align	4
 L$ecb_dec_four:
-	call	_aesni_decrypt4
+	call	__aesni_decrypt4
 	movups	%xmm2,(%rsi)
 	pxor	%xmm2,%xmm2
 	movups	%xmm3,16(%rsi)
@@ -836,7 +836,7 @@ L$ecb_dec_four:
 .p2align	4
 L$ecb_dec_five:
 	xorps	%xmm7,%xmm7
-	call	_aesni_decrypt6
+	call	__aesni_decrypt6
 	movups	%xmm2,(%rsi)
 	pxor	%xmm2,%xmm2
 	movups	%xmm3,16(%rsi)
@@ -851,7 +851,7 @@ L$ecb_dec_five:
 	jmp	L$ecb_ret
 .p2align	4
 L$ecb_dec_six:
-	call	_aesni_decrypt6
+	call	__aesni_decrypt6
 	movups	%xmm2,(%rsi)
 	pxor	%xmm2,%xmm2
 	movups	%xmm3,16(%rsi)
@@ -870,10 +870,10 @@ L$ecb_ret:
 	pxor	%xmm1,%xmm1
 	.byte	0xf3,0xc3
 
-.globl	_aesni_ccm64_encrypt_blocks
+.globl	__aesni_ccm64_encrypt_blocks
 
 .p2align	4
-_aesni_ccm64_encrypt_blocks:
+__aesni_ccm64_encrypt_blocks:
 	movl	240(%rcx),%eax
 	movdqu	(%r8),%xmm6
 	movdqa	L$increment64(%rip),%xmm9
@@ -933,10 +933,10 @@ L$ccm64_enc2_loop:
 	pxor	%xmm6,%xmm6
 	.byte	0xf3,0xc3
 
-.globl	_aesni_ccm64_decrypt_blocks
+.globl	__aesni_ccm64_decrypt_blocks
 
 .p2align	4
-_aesni_ccm64_decrypt_blocks:
+__aesni_ccm64_decrypt_blocks:
 	movl	240(%rcx),%eax
 	movups	(%r8),%xmm6
 	movdqu	(%r9),%xmm3
@@ -1030,10 +1030,10 @@ L$oop_enc1_6:
 	pxor	%xmm6,%xmm6
 	.byte	0xf3,0xc3
 
-.globl	_aesni_ctr32_encrypt_blocks
+.globl	__aesni_ctr32_encrypt_blocks
 
 .p2align	4
-_aesni_ctr32_encrypt_blocks:
+__aesni_ctr32_encrypt_blocks:
 	cmpq	$1,%rdx
 	jne	L$ctr32_bulk
 
@@ -1603,10 +1603,10 @@ L$ctr32_done:
 L$ctr32_epilogue:
 	.byte	0xf3,0xc3
 
-.globl	_aesni_xts_encrypt
+.globl	__aesni_xts_encrypt
 
 .p2align	4
-_aesni_xts_encrypt:
+__aesni_xts_encrypt:
 	leaq	(%rsp),%rax
 	pushq	%rbp
 	subq	$112,%rsp
@@ -1899,7 +1899,7 @@ L$xts_enc_short:
 	pxor	%xmm14,%xmm6
 	pxor	%xmm7,%xmm7
 
-	call	_aesni_encrypt6
+	call	__aesni_encrypt6
 
 	xorps	%xmm10,%xmm2
 	movdqa	%xmm15,%xmm10
@@ -1945,7 +1945,7 @@ L$xts_enc_two:
 	xorps	%xmm10,%xmm2
 	xorps	%xmm11,%xmm3
 
-	call	_aesni_encrypt2
+	call	__aesni_encrypt2
 
 	xorps	%xmm10,%xmm2
 	movdqa	%xmm12,%xmm10
@@ -1965,7 +1965,7 @@ L$xts_enc_three:
 	xorps	%xmm11,%xmm3
 	xorps	%xmm12,%xmm4
 
-	call	_aesni_encrypt3
+	call	__aesni_encrypt3
 
 	xorps	%xmm10,%xmm2
 	movdqa	%xmm13,%xmm10
@@ -1989,7 +1989,7 @@ L$xts_enc_four:
 	xorps	%xmm12,%xmm4
 	xorps	%xmm13,%xmm5
 
-	call	_aesni_encrypt4
+	call	__aesni_encrypt4
 
 	pxor	%xmm10,%xmm2
 	movdqa	%xmm14,%xmm10
@@ -2068,10 +2068,10 @@ L$xts_enc_ret:
 L$xts_enc_epilogue:
 	.byte	0xf3,0xc3
 
-.globl	_aesni_xts_decrypt
+.globl	__aesni_xts_decrypt
 
 .p2align	4
-_aesni_xts_decrypt:
+__aesni_xts_decrypt:
 	leaq	(%rsp),%rax
 	pushq	%rbp
 	subq	$112,%rsp
@@ -2369,7 +2369,7 @@ L$xts_dec_short:
 	pxor	%xmm13,%xmm5
 	pxor	%xmm14,%xmm6
 
-	call	_aesni_decrypt6
+	call	__aesni_decrypt6
 
 	xorps	%xmm10,%xmm2
 	xorps	%xmm11,%xmm3
@@ -2425,7 +2425,7 @@ L$xts_dec_two:
 	xorps	%xmm10,%xmm2
 	xorps	%xmm11,%xmm3
 
-	call	_aesni_decrypt2
+	call	__aesni_decrypt2
 
 	xorps	%xmm10,%xmm2
 	movdqa	%xmm12,%xmm10
@@ -2446,7 +2446,7 @@ L$xts_dec_three:
 	xorps	%xmm11,%xmm3
 	xorps	%xmm12,%xmm4
 
-	call	_aesni_decrypt3
+	call	__aesni_decrypt3
 
 	xorps	%xmm10,%xmm2
 	movdqa	%xmm13,%xmm10
@@ -2471,7 +2471,7 @@ L$xts_dec_four:
 	xorps	%xmm12,%xmm4
 	xorps	%xmm13,%xmm5
 
-	call	_aesni_decrypt4
+	call	__aesni_decrypt4
 
 	pxor	%xmm10,%xmm2
 	movdqa	%xmm14,%xmm10
@@ -2570,10 +2570,10 @@ L$xts_dec_ret:
 L$xts_dec_epilogue:
 	.byte	0xf3,0xc3
 
-.globl	_aesni_cbc_encrypt
+.globl	__aesni_cbc_encrypt
 
 .p2align	4
-_aesni_cbc_encrypt:
+__aesni_cbc_encrypt:
 	testq	%rdx,%rdx
 	jz	L$cbc_ret
 
@@ -2914,7 +2914,7 @@ L$cbc_dec_six_or_seven:
 	ja	L$cbc_dec_seven
 
 	movaps	%xmm7,%xmm8
-	call	_aesni_decrypt6
+	call	__aesni_decrypt6
 	pxor	%xmm10,%xmm2
 	movaps	%xmm8,%xmm10
 	pxor	%xmm11,%xmm3
@@ -2940,7 +2940,7 @@ L$cbc_dec_six_or_seven:
 L$cbc_dec_seven:
 	movups	96(%rdi),%xmm8
 	xorps	%xmm9,%xmm9
-	call	_aesni_decrypt8
+	call	__aesni_decrypt8
 	movups	80(%rdi),%xmm9
 	pxor	%xmm10,%xmm2
 	movups	96(%rdi),%xmm10
@@ -2986,7 +2986,7 @@ L$cbc_dec_loop6_enter:
 	leaq	96(%rdi),%rdi
 	movdqa	%xmm7,%xmm8
 
-	call	_aesni_decrypt6
+	call	__aesni_decrypt6
 
 	pxor	%xmm10,%xmm2
 	movdqa	%xmm8,%xmm10
@@ -3036,7 +3036,7 @@ L$cbc_dec_tail:
 	movaps	%xmm5,%xmm14
 	movaps	%xmm6,%xmm15
 	xorps	%xmm7,%xmm7
-	call	_aesni_decrypt6
+	call	__aesni_decrypt6
 	pxor	%xmm10,%xmm2
 	movaps	%xmm15,%xmm10
 	pxor	%xmm11,%xmm3
@@ -3077,7 +3077,7 @@ L$oop_dec1_17:
 .p2align	4
 L$cbc_dec_two:
 	movaps	%xmm3,%xmm12
-	call	_aesni_decrypt2
+	call	__aesni_decrypt2
 	pxor	%xmm10,%xmm2
 	movaps	%xmm12,%xmm10
 	pxor	%xmm11,%xmm3
@@ -3089,7 +3089,7 @@ L$cbc_dec_two:
 .p2align	4
 L$cbc_dec_three:
 	movaps	%xmm4,%xmm13
-	call	_aesni_decrypt3
+	call	__aesni_decrypt3
 	pxor	%xmm10,%xmm2
 	movaps	%xmm13,%xmm10
 	pxor	%xmm11,%xmm3
@@ -3104,7 +3104,7 @@ L$cbc_dec_three:
 .p2align	4
 L$cbc_dec_four:
 	movaps	%xmm5,%xmm14
-	call	_aesni_decrypt4
+	call	__aesni_decrypt4
 	pxor	%xmm10,%xmm2
 	movaps	%xmm14,%xmm10
 	pxor	%xmm11,%xmm3
@@ -3155,12 +3155,12 @@ L$cbc_dec_ret:
 L$cbc_ret:
 	.byte	0xf3,0xc3
 
-.globl	_aesni_set_decrypt_key
+.globl	__aesni_set_decrypt_key
 
 .p2align	4
-_aesni_set_decrypt_key:
+__aesni_set_decrypt_key:
 .byte	0x48,0x83,0xEC,0x08
-	call	__aesni_set_encrypt_key
+	call	___aesni_set_encrypt_key
 	shll	$4,%esi
 	testl	%eax,%eax
 	jnz	L$dec_key_ret
@@ -3195,11 +3195,11 @@ L$dec_key_ret:
 	.byte	0xf3,0xc3
 L$SEH_end_set_decrypt_key:
 
-.globl	_aesni_set_encrypt_key
+.globl	__aesni_set_encrypt_key
 
 .p2align	4
-_aesni_set_encrypt_key:
 __aesni_set_encrypt_key:
+___aesni_set_encrypt_key:
 .byte	0x48,0x83,0xEC,0x08
 	movq	$-1,%rax
 	testq	%rdi,%rdi
diff --git a/lib/accelerated/x86/macosx/sha1-ssse3-x86.s b/lib/accelerated/x86/macosx/sha1-ssse3-x86.s
index 8e01010..c1c3546 100644
--- a/lib/accelerated/x86/macosx/sha1-ssse3-x86.s
+++ b/lib/accelerated/x86/macosx/sha1-ssse3-x86.s
@@ -39,10 +39,10 @@
 #
 .file	"sha1-586.s"
 .text
-.globl	_sha1_block_data_order
+.globl	__sha1_block_data_order
 .align	4
-_sha1_block_data_order:
-L_sha1_block_data_order_begin:
+__sha1_block_data_order:
+L__sha1_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
diff --git a/lib/accelerated/x86/macosx/sha1-ssse3-x86_64.s b/lib/accelerated/x86/macosx/sha1-ssse3-x86_64.s
index 79c10de..dc1e16d 100644
--- a/lib/accelerated/x86/macosx/sha1-ssse3-x86_64.s
+++ b/lib/accelerated/x86/macosx/sha1-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	_sha1_block_data_order
+.globl	__sha1_block_data_order
 
 .p2align	4
-_sha1_block_data_order:
+__sha1_block_data_order:
 	movl	__gnutls_x86_cpuid_s+0(%rip),%r9d
 	movl	__gnutls_x86_cpuid_s+4(%rip),%r8d
 	testl	$512,%r8d
@@ -1329,7 +1329,7 @@ L$epilogue:
 
 
 .p2align	4
-sha1_block_data_order_ssse3:
+__sha1_block_data_order_ssse3:
 _ssse3_shortcut:
 	pushq	%rbx
 	pushq	%rbp
diff --git a/lib/accelerated/x86/macosx/sha256-ssse3-x86.s b/lib/accelerated/x86/macosx/sha256-ssse3-x86.s
index 300212c..469ed47 100644
--- a/lib/accelerated/x86/macosx/sha256-ssse3-x86.s
+++ b/lib/accelerated/x86/macosx/sha256-ssse3-x86.s
@@ -39,10 +39,10 @@
 #
 .file	"sha512-586.s"
 .text
-.globl	_sha256_block_data_order
+.globl	__sha256_block_data_order
 .align	4
-_sha256_block_data_order:
-L_sha256_block_data_order_begin:
+__sha256_block_data_order:
+L__sha256_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
 	pushl	%esi
diff --git a/lib/accelerated/x86/macosx/sha512-ssse3-x86_64.s b/lib/accelerated/x86/macosx/sha512-ssse3-x86_64.s
index 7e73227..5ff6af1 100644
--- a/lib/accelerated/x86/macosx/sha512-ssse3-x86_64.s
+++ b/lib/accelerated/x86/macosx/sha512-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	_sha256_block_data_order
+.globl	__sha256_block_data_order
 
 .p2align	4
-_sha256_block_data_order:
+__sha256_block_data_order:
 	leaq	__gnutls_x86_cpuid_s(%rip),%r11
 	movl	0(%r11),%r9d
 	movl	4(%r11),%r10d
@@ -1801,7 +1801,7 @@ K256:
 .byte	83,72,65,50,53,54,32,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
 
 .p2align	6
-sha256_block_data_order_ssse3:
+__sha256_block_data_order_ssse3:
 L$ssse3_shortcut:
 	pushq	%rbx
 	pushq	%rbp
diff --git a/lib/accelerated/x86/sha-x86-ssse3.c b/lib/accelerated/x86/sha-x86-ssse3.c
index d73039e..ec7f7c0 100644
--- a/lib/accelerated/x86/sha-x86-ssse3.c
+++ b/lib/accelerated/x86/sha-x86-ssse3.c
@@ -31,9 +31,9 @@
 #include <sha-x86.h>
 #include <x86-common.h>
 
-void sha1_block_data_order(void *c, const void *p, size_t len);
-void sha256_block_data_order(void *c, const void *p, size_t len);
-void sha512_block_data_order(void *c, const void *p, size_t len);
+void _sha1_block_data_order(void *c, const void *p, size_t len);
+void _sha256_block_data_order(void *c, const void *p, size_t len);
+void _sha512_block_data_order(void *c, const void *p, size_t len);
 
 typedef void (*update_func) (void *, size_t, const uint8_t *);
 typedef void (*digest_func) (void *, size_t, uint8_t *);
@@ -110,7 +110,7 @@ void x86_sha1_update(struct sha1_ctx *ctx, size_t length,
 
 		t2 = length / SHA1_DATA_SIZE;
 
-		sha1_block_data_order(&octx, data, t2);
+		_sha1_block_data_order(&octx, data, t2);
 
 		for (i=0;i<t2;i++)
 			ctx->count++;
@@ -163,7 +163,7 @@ void x86_sha256_update(struct sha256_ctx *ctx, size_t length,
 
 	if (length > 0) {
 		t2 = length / SHA1_DATA_SIZE;
-		sha256_block_data_order(&octx, data, t2);
+		_sha256_block_data_order(&octx, data, t2);
 		
 		for (i=0;i<t2;i++)
 			ctx->count++;
@@ -215,7 +215,7 @@ void x86_sha512_update(struct sha512_ctx *ctx, size_t length,
 
 	if (length > 0) {
 		t2 = length / SHA512_DATA_SIZE;
-		sha512_block_data_order(&octx, data, t2);
+		_sha512_block_data_order(&octx, data, t2);
 		
 		for (i=0;i<t2;i++)
 			MD_INCR(ctx);
-- 
2.14.3 (Apple Git-98)

